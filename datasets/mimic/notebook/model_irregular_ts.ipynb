{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0625f98c-4825-40fb-844b-c5c00e978087",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e20cba7c-70a7-4243-950b-dedeb8ca9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b2d0d10-9fe6-4fa8-8433-7a195580a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3fc71-ea0b-4e18-8ac6-1faa65bc4e75",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6cac6c-bba8-4b8a-9ed3-7b2c9032b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class VitalsDataset(Dataset):\n",
    "    def __init__(self, processed_dir: str, train: bool):\n",
    "        self.processed_dir = processed_dir\n",
    "\n",
    "        if train:\n",
    "            self.data_path = os.path.join(self.processed_dir, 'train/')\n",
    "            self.index_path = os.path.join(\n",
    "                self.processed_dir, 'train_idxs.npy')\n",
    "        else:\n",
    "            self.data_path = os.path.join(self.processed_dir, 'test/')\n",
    "            self.index_path = os.path.join(self.processed_dir, 'test_idxs.npy')\n",
    "\n",
    "        try:\n",
    "            self.idxs = np.load(self.index_path)\n",
    "            self.vitals = pd.read_csv(\n",
    "                os.path.join(self.data_path, 'vitals.csv'))\n",
    "            self.labels = pd.read_csv(os.path.join(self.data_path, 'labels.csv'))\n",
    "        except FileNotFoundError as e:\n",
    "            print(\"Make sure data has been processed: \", e)\n",
    "\n",
    "        self.vitals.set_index(['pat_id', 'hours_in'], inplace=True)\n",
    "        self.labels.set_index('pat_id', inplace=True)\n",
    "\n",
    "        self.num_feats = self.vitals.shape[1]\n",
    "        self.num_classes = self.labels.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pat_id = self.idxs[idx]\n",
    "\n",
    "        vit = self.vitals.loc[pat_id]\n",
    "        vit = self._format_ts_batch(vit)\n",
    "\n",
    "        lbl = self.labels.loc[pat_id].values\n",
    "\n",
    "        vit = torch.from_numpy(vit).float()\n",
    "        lbl = torch.from_numpy(lbl)\n",
    "\n",
    "        return vit, lbl\n",
    "\n",
    "    def _format_ts_batch(self, batch_ts_df):\n",
    "        if batch_ts_df.index.nlevels == 1:\n",
    "            return batch_ts_df.values\n",
    "            \n",
    "        batch_ts = batch_ts_df.groupby(level=0).apply(lambda x: x.values).values.tolist()\n",
    "        max_seq_len = max([seq.shape[0] for seq in batch_ts])\n",
    "    \n",
    "        for i, seq in enumerate(batch_ts):\n",
    "            null_rows = np.zeros((max_seq_len-seq.shape[0], self.num_feats))\n",
    "            batch_ts[i] = np.vstack([seq, null_rows])\n",
    "    \n",
    "        return np.array(batch_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251ecdea-22ef-4785-8b8a-0b4589e49e5a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "57fa874b-8528-4224-bac9-26100e1a1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, lstm_layers, dense_units):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=lstm_layers, batch_first=True)\n",
    "\n",
    "        self.dense_layers = nn.Sequential()\n",
    "        dense_units = [hidden_size] + dense_units\n",
    "        num_dense_layers = len(dense_units)-1\n",
    "        \n",
    "        for dl_idx in range(num_dense_layers):\n",
    "            inp_shape, out_shape = dense_units[dl_idx:dl_idx+2]\n",
    "            self.dense_layers.append(nn.Linear(inp_shape, out_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lstm(x)[0]\n",
    "        x = self.dense_layers(x)\n",
    "\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c16e5-b258-44b3-a29d-011aa7b21df1",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e94855-771f-4624-ac14-53b04d463320",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "628570a5-bae4-4553-a6a4-eab9a04ed31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = VitalsDataset('../data/processed', True)\n",
    "test_ds = VitalsDataset('../data/processed', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439bc79-bdc6-4608-9a80-dc52e10c078b",
   "metadata": {},
   "source": [
    "Select the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "46588c48-f7dc-4173-b1c9-33b01572b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74745e1-9057-42e1-ae3b-067701fc98d0",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e7134e24-37dc-4434-aa93-81391929db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE=32\n",
    "NUM_LAYERS=2\n",
    "\n",
    "model = TimeSeriesModel(train_ds.vitals.shape[1], HIDDEN_SIZE, NUM_LAYERS, [16,8,2]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf2a4f-dd9f-4892-a906-e150b79d56c8",
   "metadata": {},
   "source": [
    "Define constants and utilities for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e609cf24-e175-4b28-9742-df653a77cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "WRITE_FREQ = 50\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3bc7c2-bdca-4ee8-a007-6f30b20a6e5a",
   "metadata": {},
   "source": [
    "Determine indices for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "99817719-b288-4c92-a8d5-e5232157f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "KFOLDS = 7\n",
    "\n",
    "train_idxs = np.arange(0,len(train_ds))\n",
    "\n",
    "kfolder = KFold(KFOLDS, shuffle=True)\n",
    "split_idxs = list(kfolder.split(train_idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3142560-3e66-4a81-afc7-3d270f566d38",
   "metadata": {},
   "source": [
    "A single training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1025f5ef-6b62-4e1c-a0cb-fee0d9f47751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_idxs, epoch_idx):\n",
    "    num_batches = math.ceil(len(train_idxs)/BATCH_SIZE)\n",
    "\n",
    "    running_loss, last_loss = 0, 0\n",
    "    \n",
    "    # Loop through each batch\n",
    "    for batch_idx in range(num_batches):\n",
    "        idxs = train_idxs[batch_idx*BATCH_SIZE:min((batch_idx+1)*BATCH_SIZE, len(train_idxs))]\n",
    "        inputs, labels = train_ds[idxs]\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % WRITE_FREQ == WRITE_FREQ-1:\n",
    "            last_loss = running_loss / WRITE_FREQ\n",
    "            print(f\"\\t batch {batch_idx+1} loss: {last_loss}\")\n",
    "            running_loss = 0\n",
    "            log_idx = (epoch_idx * num_batches) + batch_idx + 1\n",
    "            writer.add_scalar('Loss/Train', last_loss, log_idx)\n",
    "            \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcba6f0-068c-4315-8a70-932f1246da27",
   "metadata": {},
   "source": [
    "A single validation epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e2872627-f81e-4090-aff5-5d53fbc35569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(val_idxs, epoch_idx):\n",
    "    model.eval()\n",
    "    \n",
    "    num_batches = math.ceil(len(val_idxs)/BATCH_SIZE)\n",
    "    \n",
    "    running_vloss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(num_batches):\n",
    "            idxs = val_idxs[batch_idx*BATCH_SIZE:min((batch_idx+1)*BATCH_SIZE, len(val_idxs))]\n",
    "            vinputs, vlabels = train_ds[idxs]\n",
    "            vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "    \n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "    \n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / num_batches\n",
    "    writer.add_scalar('Loss/Valid', avg_vloss, epoch_idx+1)\n",
    "    \n",
    "    return avg_vloss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b732289-e532-4cb1-bb0a-750d180b70f5",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1fd96c20-14bf-4789-abde-f1912b61ad22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\t batch 50 loss: 4.274215307235718\n",
      "\t batch 100 loss: 0.8940871798992157\n",
      "\t batch 150 loss: 0.665357985496521\n",
      "\t batch 200 loss: 0.4936798173189163\n",
      "\t batch 250 loss: 0.34500348269939424\n",
      "\t batch 300 loss: 0.3088918437063694\n",
      "\t batch 350 loss: 0.33345985144376755\n",
      "\t batch 400 loss: 0.3173807245492935\n",
      "\t batch 450 loss: 0.29585755854845047\n",
      "\t batch 500 loss: 0.3209942053258419\n",
      "\t batch 550 loss: 0.35591398358345033\n",
      "\t batch 600 loss: 0.31334610626101495\n",
      "\t batch 650 loss: 0.3096887290477753\n",
      "\t batch 700 loss: 0.3063186648488045\n",
      "LOSS train 0.3063186648488045 valid 0.31509333848953247\n",
      "Epoch 2\n",
      "\t batch 50 loss: 0.32112288117408755\n",
      "\t batch 100 loss: 0.3003802926838398\n",
      "\t batch 150 loss: 0.31788292616605757\n",
      "\t batch 200 loss: 0.29329873085021974\n",
      "\t batch 250 loss: 0.3240022474527359\n",
      "\t batch 300 loss: 0.30495404794812203\n",
      "\t batch 350 loss: 0.3206598922610283\n",
      "\t batch 400 loss: 0.30912388265132906\n",
      "\t batch 450 loss: 0.2967716883122921\n",
      "\t batch 500 loss: 0.3163670578598976\n",
      "\t batch 550 loss: 0.35370143204927446\n",
      "\t batch 600 loss: 0.31200670570135114\n",
      "\t batch 650 loss: 0.30092471331357956\n",
      "\t batch 700 loss: 0.30840583115816117\n",
      "LOSS train 0.30840583115816117 valid 0.3194787800312042\n",
      "Epoch 3\n",
      "\t batch 50 loss: 0.3256921777129173\n",
      "\t batch 100 loss: 0.31575647339224816\n",
      "\t batch 150 loss: 0.32475834503769874\n",
      "\t batch 200 loss: 0.2923924906551838\n",
      "\t batch 250 loss: 0.30777793169021606\n",
      "\t batch 300 loss: 0.305514774620533\n",
      "\t batch 350 loss: 0.31209861814975737\n",
      "\t batch 400 loss: 0.31055507093667983\n",
      "\t batch 450 loss: 0.29068506181240084\n",
      "\t batch 500 loss: 0.31268206164240836\n",
      "\t batch 550 loss: 0.36006206408143043\n",
      "\t batch 600 loss: 0.3095914414525032\n",
      "\t batch 650 loss: 0.30554906189441683\n",
      "\t batch 700 loss: 0.30807980731129647\n",
      "LOSS train 0.30807980731129647 valid 0.31529366970062256\n",
      "Epoch 4\n",
      "\t batch 50 loss: 0.30939465343952177\n",
      "\t batch 100 loss: 0.30677622377872465\n",
      "\t batch 150 loss: 0.32310173988342283\n",
      "\t batch 200 loss: 0.29589982092380523\n",
      "\t batch 250 loss: 0.30647665560245513\n",
      "\t batch 300 loss: 0.31597019225358963\n",
      "\t batch 350 loss: 0.32372795209288596\n",
      "\t batch 400 loss: 0.31159451439976693\n",
      "\t batch 450 loss: 0.2877918489277363\n",
      "\t batch 500 loss: 0.31544149428606033\n",
      "\t batch 550 loss: 0.3707955366373062\n",
      "\t batch 600 loss: 0.3286163273453713\n",
      "\t batch 650 loss: 0.3066757422685623\n",
      "\t batch 700 loss: 0.30222007125616074\n",
      "LOSS train 0.30222007125616074 valid 0.307544469833374\n",
      "Epoch 5\n",
      "\t batch 50 loss: 0.3213439138233662\n",
      "\t batch 100 loss: 0.30933732628822325\n",
      "\t batch 150 loss: 0.3260465905070305\n",
      "\t batch 200 loss: 0.28722323015332224\n",
      "\t batch 250 loss: 0.3277427998185158\n",
      "\t batch 300 loss: 0.30625807464122773\n",
      "\t batch 350 loss: 0.3223933674395084\n",
      "\t batch 400 loss: 0.31751537263393403\n",
      "\t batch 450 loss: 0.29824380844831466\n",
      "\t batch 500 loss: 0.3146901249885559\n",
      "\t batch 550 loss: 0.36453806191682814\n",
      "\t batch 600 loss: 0.29788794040679933\n",
      "\t batch 650 loss: 0.31387416884303093\n",
      "\t batch 700 loss: 0.2993528935313225\n",
      "LOSS train 0.2993528935313225 valid 0.3049696981906891\n"
     ]
    }
   ],
   "source": [
    "best_vloss = 1e9\n",
    "\n",
    "for epoch_idx in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch_idx+1}\")\n",
    "\n",
    "    kfold_idx = epoch_idx % KFOLDS\n",
    "    train_idxs, val_idxs = split_idxs[kfold_idx] \n",
    "    \n",
    "    model.train(True)\n",
    "    avg_loss = train_epoch(train_idxs, epoch_idx)\n",
    "\n",
    "    avg_vloss = val_epoch(val_idxs, epoch_idx)\n",
    "    print(f'LOSS train {avg_loss} valid {avg_vloss}')\n",
    "\n",
    "    writer.add_scalars('Training vs Validation Loss', {'Training': avg_loss, 'Validation': avg_vloss}, epoch_idx+1)\n",
    "    writer.flush()\n",
    "\n",
    "    if avg_vloss < best_vloss:\n",
    "        bst_vloss = avg_vloss\n",
    "        model_path = f\"model_{epoch_idx}\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598aa94b-de76-4deb-908f-24c6479eb9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic3",
   "language": "python",
   "name": "mimic3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
