{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e4dfcc-9748-49fd-9d08-bde0e977ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099dc20f-88c3-497e-8a00-1602c65e1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_TOKEN = '[SEP]'\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self):\n",
    "        self.tok2id = {}\n",
    "        self.id2tok = {}\n",
    "        self.tok2cnt = {}\n",
    "        self.cnt = 1\n",
    "\n",
    "    def add_token(self, token: str):\n",
    "        if token not in self.tok2id:\n",
    "            self.tok2id[token] = self.cnt\n",
    "            self.id2tok[self.cnt] = token\n",
    "            self.tok2cnt[token] = 1\n",
    "            self.cnt += 1\n",
    "        else:\n",
    "            self.tok2cnt[token] += 1\n",
    "\n",
    "    def top_tokens(self, top: int):\n",
    "        return set([tok for _, tok in sorted(list({v:k for k,v in self.tok2cnt.items()}.items()), reverse=True)[:top]])\n",
    "\n",
    "    def to_json(self, path: str):\n",
    "        vocab_data = {\n",
    "            'tok2id': self.tok2id,\n",
    "            'id2tok': self.id2tok,\n",
    "            'tok2cnt': self.tok2cnt,\n",
    "            'cnt': self.cnt\n",
    "        }\n",
    "\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(vocab_data, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, path: str):\n",
    "        with open(path, 'r') as f:\n",
    "            vocab_data = json.load(f)\n",
    "\n",
    "        v = Vocab()\n",
    "        v.tok2id = {k: int(v) for k,v in vocab_data['tok2id'].items()}\n",
    "        v.id2tok = {int(k): v for k,v in vocab_data['id2tok'].items()}\n",
    "        v.tok2cnt = {k: int(v) for k,v in vocab_data['tok2cnt'].items()}\n",
    "        v.cnt = vocab_data['cnt']\n",
    "\n",
    "        return v\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "983294ea-d841-47e9-8ec6-92a0f0ef0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICDataset(Dataset):\n",
    "    def __init__(self, processed_dir: str, train: bool):\n",
    "        self.processed_dir = processed_dir\n",
    "\n",
    "        data_path = os.path.join(processed_dir, ('train' if train else 'test'))\n",
    "        index_path = os.path.join(processed_dir, f\"{'train' if train else 'test'}_idxs.npy\")\n",
    "\n",
    "        try:\n",
    "            self.indexes = np.load(index_path)\n",
    "            self.demographics = pd.read_csv(os.path.join(data_path, 'demographic.csv'))\n",
    "            self.vitals = pd.read_csv(os.path.join(data_path, 'vitals.csv'))\n",
    "            self.interventions = pd.read_csv(os.path.join(data_path, 'interventions.csv'))\n",
    "\n",
    "            self.vocab = Vocab.from_json(os.path.join(processed_dir, 'vocab.json')) \n",
    "            self.notes_static_path = os.path.join(data_path, 'notes_static.h5')\n",
    "            self.notes_ts_path = os.path.join(data_path, 'notes_ts.h5')\n",
    "        except FileNotFoundError as e:\n",
    "            print(\"Make sure data has been processed: \", e)\n",
    "            return\n",
    "\n",
    "        self.demographics.set_index('pat_id', inplace=True)\n",
    "        self.vitals.set_index(['pat_id', 'hours_in'], inplace=True)\n",
    "        self.interventions.set_index(['pat_id', 'hours_in'], inplace=True)\n",
    "\n",
    "        with h5py.File(self.notes_static_path, 'r') as f:\n",
    "            self.nst_ids = set([int(k.split('_')[-1]) for k in list(f.keys())])\n",
    "        with h5py.File(self.notes_ts_path, 'r') as f:\n",
    "            self.nts_ids = set([int(k.split('_')[-1]) for k in list(f.keys())])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indexes)\n",
    "\n",
    "    def __getitem__(self, item_idx):\n",
    "        pat_id = self.indexes[item_idx]\n",
    "\n",
    "        dem = self.demographics.loc[pat_id]\n",
    "\n",
    "        vit = self.vitals.loc[pat_id]\n",
    "        vit = self._format_ts_batch(vit)\n",
    "\n",
    "        itv = self.interventions.loc[pat_id]\n",
    "        itv = self._format_ts_batch(itv)\n",
    "\n",
    "        if type(pat_id) != np.ndarray:\n",
    "            nst, nts, missing = self._getpatient_notes(pat_id)\n",
    "        else:\n",
    "            nst, nts, missing = self._getpatients_notes(pat_id)\n",
    "        \n",
    "        return dem, vit, itv, nst, nts, missing\n",
    "\n",
    "    def _getpatient_notes(self, pat_id):\n",
    "        nst, nts = np.empty(0), (np.empty(0), np.empty(0), np.empty(0))\n",
    "        missing = [True, True]\n",
    "\n",
    "        if pat_id in self.nst_ids:\n",
    "            with h5py.File(self.notes_static_path, 'r') as f:\n",
    "                nst = f[f'row_{pat_id}']\n",
    "                missing[0] = False\n",
    "            \n",
    "        if pat_id in self.nts_ids:\n",
    "            with h5py.File(self.notes_ts_path, 'r') as f:\n",
    "                nts = self._format_notes_ts_group(f[f'pat_id_{pat_id}'])\n",
    "                missing[1] = False\n",
    "\n",
    "        return nst, nts, missing\n",
    "\n",
    "    def _getpatients_notes(self, pat_ids):\n",
    "        nst, nts = [], []\n",
    "        missing = []\n",
    "\n",
    "        missing_st = []\n",
    "        match_ids = set([pat_id for pat_id in pat_ids if pat_id in self.nst_ids])\n",
    "        with h5py.File(self.notes_static_path, 'r') as f:\n",
    "            for pat_id in pat_ids:\n",
    "                if pat_id in match_ids:\n",
    "                    nst.append(f[f'row_{pat_id}'])\n",
    "                    missing_st.append(False)\n",
    "                else:\n",
    "                    missing_st.append(True)\n",
    "\n",
    "        missing_ts = []\n",
    "        match_ids = set([pat_id for pat_id in pat_ids if pat_id in self.nts_ids])\n",
    "        with h5py.File(self.notes_ts_path, 'r') as f:\n",
    "            for pat_id in pat_ids:\n",
    "                if pat_id in match_ids:\n",
    "                    nst.append(self._format_notes_ts_group(f[f'pat_id_{pat_id}']))\n",
    "                    missing_ts.append(False)\n",
    "                else:\n",
    "                    missing_ts.append(True)\n",
    "\n",
    "        missing = np.array(list(zip(missing_st, missing_ts)))\n",
    "\n",
    "        return nst, nts, missing\n",
    "    \n",
    "    @staticmethod\n",
    "    def _format_ts_batch(batch_ts_df):\n",
    "        if batch_ts_df.index.nlevels == 1:\n",
    "            return batch_ts_df.values\n",
    "            \n",
    "        batch_ts = batch_ts_df.groupby(level=0).apply(lambda x: x.values).values.tolist()\n",
    "        max_seq_len = max([seq.shape[0] for seq in batch_ts])\n",
    "    \n",
    "        for i, seq in enumerate(batch_ts):\n",
    "            null_rows = np.zeros((max_seq_len-seq.shape[0], batch_ts_df.shape[1]))\n",
    "            batch_ts[i] = np.vstack([seq, null_rows])\n",
    "    \n",
    "        return np.array(batch_ts)\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_notes_ts_group(nts_group):\n",
    "        group_size = len(nts_group)\n",
    "        times, cats, notes = [0]*group_size, [0]*group_size, [0]*group_size\n",
    "        for d in nts_group.keys():\n",
    "            _, gidx, _, time, _, cat = d.split('_')\n",
    "            gidx, time, cat = int(gidx), int(time), np.array([int(c) for c in cat])\n",
    "            times[gidx] = time\n",
    "            cats[gidx] = cat\n",
    "            notes[gidx] = nts_group[d][:]\n",
    "\n",
    "        times, cats = np.array(times), np.array(cats)\n",
    "        \n",
    "        max_note_len = max([len(note) for note in notes])\n",
    "        notes = np.array([np.pad(note, (0, max_note_len-len(note))) for note in notes])\n",
    "\n",
    "        return times, cats, notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c550aa5-3466-4a97-97a4-50adc0de6b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MIMICDataset('../data/processed', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71f0ab32-bab8-445f-b466-e61af4dd2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MIMICDataset('../data/processed', True)\n",
    "test_ds = MIMICDataset('../data/processed', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c928a3b-056b-4f89-aaeb-78f6c612c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c38b3211-f96f-439c-8a41-3ea6eefa288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738cf66a-fc68-48ee-aad3-58364bfa671c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MIMICDataset at 0x1c67648f370>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "deb78843-d0a6-4e09-8872-473b9c8bb040",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "400e0648-fa66-48ed-aa79-705865104221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<zip object at 0x0000028646BAEC40>, dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(zip(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed22de-7511-4650-82ad-f284cada374e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic3",
   "language": "python",
   "name": "mimic3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
