{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-III M3Care Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jason\\Work\\M3Care\\env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from mimic.models import (MLP, TimeSeriesTransformer, TimeSeriesNLEmbedder, NLEmbedder)\n",
    "from mimic.dataset import MIMICDataset\n",
    "from mimic.vocab import Vocab\n",
    "from general.m3care import M3Care\n",
    "from util.kfold import KFoldDatasetLoader\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "processed_dir = './mimic/data/processed'\n",
    "test_ds = MIMICDataset(processed_dir, False)\n",
    "vocab = Vocab.from_json(os.path.join(processed_dir, 'vocab.json'))\n",
    "\n",
    "DEM_DIM = 18\n",
    "EMB_DIM = 512\n",
    "dem_mdl = MLP(in_dim=DEM_DIM, hidden_dim=[128,192,256], out_dim=EMB_DIM, bias=True, relu=True, norm=True)\n",
    "\n",
    "DROPOUT = 0.3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "unimodal_models = nn.ModuleList([dem_mdl])\n",
    "missing_modals = [False]\n",
    "time_modals = [False]\n",
    "timesteps_modals = []\n",
    "mask_modals = [False]\n",
    "output_dim = 2\n",
    "keep_prob = 1 - DROPOUT\n",
    "\n",
    "model = M3Care(unimodal_models, missing_modals, time_modals, timesteps_modals, mask_modals, EMB_DIM, output_dim, device, keep_prob).to(device)\n",
    "\n",
    "sample = test_ds[:32]\n",
    "dem, vit, itv, nst, vit_msk, itv_msk, nst_msk, lbl = sample\n",
    "dem_ten = torch.from_numpy(dem).float().to(device)\n",
    "lbl_ten = torch.from_numpy(lbl).float().to(device)\n",
    "res = model(dem_ten, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules and define constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mimic.models import (MLP, TimeSeriesTransformer, TimeSeriesNLEmbedder, NLEmbedder)\n",
    "from mimic.dataset import MIMICDataset\n",
    "from mimic.vocab import Vocab\n",
    "from general.m3care import M3Care\n",
    "from util.kfold import KFoldDatasetLoader\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = './mimic/data/processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = MIMICDataset(processed_dir, True)\n",
    "test_ds = MIMICDataset(processed_dir, False)\n",
    "\n",
    "vocab = Vocab.from_json(os.path.join(processed_dir, 'vocab.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEM_DIM = 18\n",
    "VIT_DIM = 104\n",
    "ITV_DIM = 14\n",
    "EMB_DIM = 512\n",
    "DROPOUT = 0.3\n",
    "\n",
    "VIT_TIMESTEPS = 150\n",
    "ITV_TIMESTEPS = 150\n",
    "NTS_TIMESTEPS = 128\n",
    "\n",
    "NST_WORD_LIMIT = 10000\n",
    "NTS_WORD_LIMIT = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Unimodal Extraction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_mdl = MLP(in_dim=DEM_DIM, hidden_dim=[128,192,256], out_dim=EMB_DIM, bias=True, relu=True, norm=True)\n",
    "vit_mdl = TimeSeriesTransformer(VIT_DIM, EMB_DIM, max_len=VIT_TIMESTEPS, dropout=DROPOUT)\n",
    "itv_mdl = TimeSeriesTransformer(ITV_DIM, EMB_DIM, max_len=ITV_TIMESTEPS, dropout=DROPOUT)\n",
    "nst_mdl = NLEmbedder(vocab, 16, EMB_DIM, 8, 2048, dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate M3Care Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Unimodal feature extractor\n",
    "# unimodal_models = nn.ModuleList([dem_mdl, vit_mdl, itv_mdl, nst_mdl])\n",
    "unimodal_models = nn.ModuleList([dem_mdl])\n",
    "# Which models have missing values\n",
    "# missing_modals = [False, True, True, True]\n",
    "missing_modals = [False]\n",
    "# Which models are time-based\n",
    "# time_modals = [False, True, True, False]\n",
    "time_modals = [False]\n",
    "# For time based models what's the max sequence length\n",
    "# timesteps_modals = [150, 150]\n",
    "timesteps_modals = []\n",
    "# Which modalities have a mask\n",
    "# mask_modals = [False, True, True, True]\n",
    "mask_modals = [False]\n",
    "# Output dim that's put into out_mdl\n",
    "output_dim = 2\n",
    "# Dropout inverse\n",
    "keep_prob = 1 - DROPOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jason\\Work\\M3Care\\env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = M3Care(unimodal_models, missing_modals, time_modals, timesteps_modals, mask_modals, EMB_DIM, output_dim, device, keep_prob).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "FOLDS = 8\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_bce = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loader = KFoldDatasetLoader(test_ds, FOLDS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHES_PER_LOG = 5\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(f'runs/m3care_mimic_{timestamp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(loader):\n",
    "    dem, vit, itv, nst, vit_msk, itv_msk, nst_msk, lbl = loader.next()\n",
    "    dem_ten = torch.from_numpy(dem).float().to(device)\n",
    "    # vit_ten = torch.from_numpy(vit).float().to(device)\n",
    "    # itv_ten = torch.from_numpy(itv).float().to(device)\n",
    "    # nst_ten = torch.from_numpy(nst).float().to(device)\n",
    "    # vit_msk_ten = torch.from_numpy(vit_msk).bool().to(device)\n",
    "    # itv_msk_ten = torch.from_numpy(itv_msk).bool().to(device)\n",
    "    # nst_msk_ten = torch.from_numpy(nst_msk).bool().to(device)\n",
    "    lbl_ten = torch.from_numpy(lbl).float().to(device)\n",
    "    # return (dem_ten, vit_ten, itv_ten, nst_ten, vit_msk_ten, itv_msk_ten, nst_msk_ten), lbl_ten\n",
    "    return (dem_ten), lbl_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbatch 5 loss: 17.238424015045165\n",
      "\tbest loss: 17.238424015045165\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accuracy, precision, recall, f1 = [], [], [], []\n",
    "total_train_batches = math.floor(len(test_ds)/BATCH_SIZE)\n",
    "\n",
    "running_loss = 0.0\n",
    "y_truths, y_preds = [], []\n",
    "\n",
    "# Each epoch ones only one fold\n",
    "for epoch_idx in range(EPOCHS):\n",
    "    loader.train()\n",
    "\n",
    "    batch_idx = 1\n",
    "\n",
    "    # Iterate through each training batch\n",
    "    while not loader.end():\n",
    "        # Load batch from the loader\n",
    "        X, y = load_batch(loader)\n",
    "        y = torch.stack([~y.bool(),y.bool()], axis=1).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred, lstab = model(X, BATCH_SIZE)\n",
    "\n",
    "        loss = loss_bce(y_pred, y)\n",
    "\n",
    "        # Adjust model accordingly.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save results for single iteration\n",
    "        running_loss += loss.detach().item()\n",
    "        y_truths.append(y[:,-1].detach().cpu().numpy())\n",
    "        y_preds.append(y_pred.argmax(axis=1).detach().cpu().numpy())\n",
    "\n",
    "        # Save results for single log iteration\n",
    "        if batch_idx % BATCHES_PER_LOG == 0:\n",
    "            last_loss = running_loss / BATCHES_PER_LOG * BATCH_SIZE\n",
    "            losses.append(last_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "            y_truths_c = np.concatenate(y_truths)\n",
    "            y_preds_c = np.concatenate(y_preds)\n",
    "            accuracy.append(accuracy_score(y_truths_c, y_preds_c))\n",
    "            precision.append(precision_score(y_truths_c, y_preds_c))\n",
    "            recall.append(recall_score(y_truths_c, y_preds_c))\n",
    "            f1.append(f1_score(y_truths_c, y_preds_c))\n",
    "            y_truths.clear()\n",
    "            y_preds.clear()\n",
    "\n",
    "            # Display Results\n",
    "            print(f\"\\tbatch {batch_idx} loss: {last_loss}\")\n",
    "            print(f\"\\tbest loss: {min(losses)}\")\n",
    "\n",
    "            # Write to tensorboard\n",
    "            tb_idx = (epoch_idx * total_train_batches + batch_idx) * BATCH_SIZE\n",
    "            writer.add_scalar('Loss/train', last_loss, tb_idx)\n",
    "\n",
    "        batch_idx += 1\n",
    "\n",
    "        if batch_idx == 10:\n",
    "            break\n",
    "\n",
    "    break\n",
    "\n",
    "    loader.val()\n",
    "\n",
    "    # Iterate through each validation batch\n",
    "    while not loader.end():\n",
    "        x = loader.next()\n",
    "\n",
    "    loader.next_fold()\n",
    "    if loader.end_fold():\n",
    "        loader.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = torch.tensor([True, False, True, True])\n",
    "y = torch.tensor([True, False, False, True])\n",
    "\n",
    "pred_y = torch.stack([~pred_y, pred_y], axis=1)\n",
    "y = torch.stack([~y, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_truths_c = np.concatenate(y_truths)\n",
    "y_preds_c = np.concatenate(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07407407407407407"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_truths_c, y_preds_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4446, -2.1665],\n",
       "        [ 2.6084, -2.1263],\n",
       "        [ 2.1012, -1.8023],\n",
       "        [ 2.9684, -2.2325],\n",
       "        [ 1.9413, -1.5935],\n",
       "        [ 2.4603, -2.0854],\n",
       "        [ 2.8656, -2.1502],\n",
       "        [ 2.1422, -1.4622],\n",
       "        [ 2.7696, -1.7585],\n",
       "        [ 2.4717, -2.3189],\n",
       "        [ 2.4094, -2.0613],\n",
       "        [ 3.2527, -2.3141],\n",
       "        [ 2.9413, -2.4611],\n",
       "        [ 1.6488, -1.5808],\n",
       "        [ 1.7613, -1.6040],\n",
       "        [ 1.9012, -1.5147],\n",
       "        [ 2.5354, -1.9792],\n",
       "        [ 2.3909, -2.1359],\n",
       "        [ 2.7984, -2.2884],\n",
       "        [ 2.0891, -1.4728],\n",
       "        [ 2.8681, -2.1382],\n",
       "        [ 2.0934, -1.5964],\n",
       "        [ 3.2246, -2.3791],\n",
       "        [ 2.7333, -2.3112],\n",
       "        [ 3.4254, -3.0554],\n",
       "        [ 2.0117, -1.7837],\n",
       "        [ 3.5364, -2.7268],\n",
       "        [ 2.4244, -1.8148],\n",
       "        [ 2.2837, -1.8553],\n",
       "        [ 2.5749, -2.1558],\n",
       "        [ 2.5648, -2.3115],\n",
       "        [ 1.9434, -1.4515]], device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
