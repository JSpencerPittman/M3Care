{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-III M3Care Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules and define constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mimic.models import (MLP, TimeSeriesTransformer, TimeSeriesNLEmbedder, NLEmbedder)\n",
    "from mimic.dataset import MIMICDataset\n",
    "from mimic.vocab import Vocab\n",
    "from general.m3care import M3Care\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = './mimic/data/processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = MIMICDataset(processed_dir, True)\n",
    "test_ds = MIMICDataset(processed_dir, False)\n",
    "\n",
    "vocab = Vocab.from_json(os.path.join(processed_dir, 'vocab.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEM_DIM = 18\n",
    "VIT_DIM = 104\n",
    "ITV_DIM = 14\n",
    "EMB_DIM = 512\n",
    "DROPOUT = 0.3\n",
    "\n",
    "VIT_TIMESTEPS = 150\n",
    "ITV_TIMESTEPS = 150\n",
    "NTS_TIMESTEPS = 128\n",
    "\n",
    "NST_WORD_LIMIT = 10000\n",
    "NTS_WORD_LIMIT = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Unimodal Extraction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_mdl = MLP(in_dim=DEM_DIM, hidden_dim=[128,192,256], out_dim=EMB_DIM, bias=True, relu=True, norm=True)\n",
    "vit_mdl = TimeSeriesTransformer(VIT_DIM, EMB_DIM, max_len=VIT_TIMESTEPS, dropout=DROPOUT)\n",
    "itv_mdl = TimeSeriesTransformer(ITV_DIM, EMB_DIM, max_len=ITV_TIMESTEPS, dropout=DROPOUT)\n",
    "nst_mdl = NLEmbedder(vocab, 16, EMB_DIM, 8, 2048, dropout=DROPOUT)\n",
    "nts_mdl = TimeSeriesNLEmbedder(vocab, 16, EMB_DIM, 8, 2048, dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate M3Care Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# unimodal_models = nn.ModuleList([dem_mdl, vit_mdl, itv_mdl, nst_mdl, nts_mdl])\n",
    "unimodal_models = nn.ModuleList([dem_mdl, vit_mdl, itv_mdl, nst_mdl])\n",
    "missing_modals = [False, True, True, True]\n",
    "time_modals = [False, True, True, False]\n",
    "timesteps_modals = [150, 150]\n",
    "mask_modals = [False, True, True, True]\n",
    "output_dim = 2\n",
    "keep_prob = 1 - DROPOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jason\\Work\\M3Care\\env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = M3Care(unimodal_models, missing_modals, time_modals, timesteps_modals, mask_modals, EMB_DIM, output_dim, device, keep_prob).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_ds[:4]\n",
    "dem, vit, itv, nst, nts, vit_msk, itv_msk, nst_msk, nts_msk, lbl = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_ten = torch.from_numpy(dem).float().to(device)\n",
    "vit_ten = torch.from_numpy(vit).float().to(device)\n",
    "itv_ten = torch.from_numpy(itv).float().to(device)\n",
    "nst_ten = torch.from_numpy(nst).float().to(device)\n",
    "# nts_ten = torch.from_numpy(nts).float().to(device)\n",
    "vit_msk_ten = torch.from_numpy(vit_msk).bool().to(device)\n",
    "itv_msk_ten = torch.from_numpy(itv_msk).bool().to(device)\n",
    "nst_msk_ten = torch.from_numpy(nst_msk).bool().to(device)\n",
    "# nts_msk_ten = torch.from_numpy(nts_msk).float().to(device)\n",
    "lbl_ten = torch.from_numpy(lbl).bool().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nanCnt(x):\n",
    "    return x.isnan().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, emb = model(dem_ten, vit_ten, itv_ten, nst_ten, vit_msk_ten, itv_msk_ten, nst_msk_ten, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o, e in zip(orig, emb):\n",
    "    print(nanCnt(o), o.shape)\n",
    "    print(nanCnt(e), e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(nst_ten)\n",
    "print(nst_msk_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nst_ten\n",
    "mask = nst_msk_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask = ~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask[3,0] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False]], device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nst_mdl.word_embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nst_mdl.pos_encode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = nst_mdl.enc_layer(x, src_key_padding_mask=pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 512])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.4918,  1.2983, -0.5192,  ..., -0.6496,  0.4102,  1.9774]]],\n",
       "       device='cuda:0', grad_fn=<NanToNumBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.nan_to_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       "\n",
       "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       "\n",
       "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       "\n",
       "        [[ 0.4918,  1.2983, -0.5192,  ..., -0.6496,  0.4102,  1.9774]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
