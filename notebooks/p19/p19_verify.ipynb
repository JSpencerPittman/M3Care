{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P19 Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torcheval.metrics import BinaryAUROC, BinaryAUPRC\n",
    "\n",
    "os.chdir('../..')\n",
    "from src.raindrop.raindrop import Raindrop\n",
    "from src.p19.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data = \\\n",
    "    load_p19_data(Path('./data/p19/processed_data'), device)\n",
    "\n",
    "train_ds, val_ds, test_ds = split_p19_data(*data, device)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.raindrop.classifier import RaindropClassifier\n",
    "\n",
    "raindrop = Raindrop(num_sensors=34,\n",
    "                 obs_dim=1,\n",
    "                 obs_embed_dim=4,\n",
    "                 pe_emb_dim=16,\n",
    "                 timesteps=60,\n",
    "                 out_dim=128,\n",
    "                 num_heads=1,\n",
    "                 num_layers=1,\n",
    "                 inter_sensor_attn_dim=16,\n",
    "                 temporal_attn_dim=16,\n",
    "                 prune_rate=0.5,\n",
    "                 device=device)\n",
    "\n",
    "rd_cls = RaindropClassifier(raindrop,\n",
    "                            static_dim=6,\n",
    "                            static_proj_dim=34,\n",
    "                            cls_hidden_dim=128,\n",
    "                            classes=2).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RaindropLoss(nn.Module):\n",
    "    def __init__(self, reg_weight: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.reg_weight = reg_weight\n",
    "\n",
    "    def forward(self, predictions, targets, reg_loss):\n",
    "        ce_loss = self.ce_loss(predictions, targets)\n",
    "        # reg_loss *= self.reg_weight\n",
    "        # return ce_loss + reg_loss\n",
    "        return ce_loss\n",
    "    \n",
    "loss_fn = RaindropLoss(reg_weight=0.02)\n",
    "# loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "LOSS_TRAIN_LOG_FREQ = 100\n",
    "\n",
    "optim = torch.optim.Adam(rd_cls.parameters(), lr=0.0001)\n",
    "\n",
    "bin_auroc_metric = BinaryAUROC()\n",
    "bin_auprc_metric = BinaryAUPRC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss, running_auroc, running_auprc = 0., 0., 0.\n",
    "    last_loss, last_auroc, last_auprc = 0., 0., 0.\n",
    "\n",
    "    for i, data in enumerate(train_dl):\n",
    "        ts_inp, times, mask, static_inp, labels = data\n",
    "\n",
    "        optim.zero_grad()\n",
    "        outputs, reg_loss = rd_cls(ts_inp, times, mask, static_inp)\n",
    "        loss = loss_fn(outputs, labels, reg_loss)\n",
    "        # loss = loss_fn(outputs, labels)\n",
    "        bin_outputs = outputs.argmax(dim=-1)\n",
    "        bin_auroc_metric.update(bin_outputs, labels)\n",
    "        bin_auprc_metric.update(bin_outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_auroc += bin_auroc_metric.compute().item()\n",
    "        running_auprc += bin_auprc_metric.compute().item()\n",
    "\n",
    "        if i % LOSS_TRAIN_LOG_FREQ == LOSS_TRAIN_LOG_FREQ-1:\n",
    "            # return \n",
    "            last_loss = running_loss / LOSS_TRAIN_LOG_FREQ\n",
    "            last_auroc = running_auroc / LOSS_TRAIN_LOG_FREQ\n",
    "            last_auprc = running_auprc / LOSS_TRAIN_LOG_FREQ\n",
    "    \n",
    "            print('  batch {} loss: {} AUROC: {} AUPRC {}'.format(i + 1, last_loss, last_auroc, last_auprc))\n",
    "            \n",
    "            tb_x = epoch_index * len(train_dl) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            tb_writer.add_scalar('AUROC/train', last_auroc, tb_x)\n",
    "            tb_writer.add_scalar('AUPRC/train', last_auprc, tb_x)\n",
    "\n",
    "            running_loss, running_auroc, running_auprc = 0., 0., 0.\n",
    "\n",
    "    return last_loss, last_auroc, last_auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 0.36133876740932463 AUROC: 0.5025004083641682 AUPRC 0.043433343786746265\n",
      "  batch 200 loss: 0.35331675261259077 AUROC: 0.5343900037817123 AUPRC 0.07739564917981624\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch_number \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     10\u001b[0m rd_cls\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m avg_loss, avg_auroc, avg_auprc \u001b[38;5;241m=\u001b[39m train_one_epoch(epoch_number, writer)\n\u001b[1;32m     14\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     15\u001b[0m running_vloss, running_vauroc, running_vauprc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m\n",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m     19\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     20\u001b[0m running_auroc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bin_auroc_metric\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 21\u001b[0m running_auprc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bin_auprc_metric\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m LOSS_TRAIN_LOG_FREQ \u001b[38;5;241m==\u001b[39m LOSS_TRAIN_LOG_FREQ\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# return \u001b[39;00m\n\u001b[1;32m     25\u001b[0m     last_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m LOSS_TRAIN_LOG_FREQ\n",
      "File \u001b[0;32m~/miniconda3/envs/m3care/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/m3care/lib/python3.12/site-packages/torcheval/metrics/classification/auprc.py:129\u001b[0m, in \u001b[0;36mBinaryAUPRC.compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode()\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m: TBinaryAUPRC,\n\u001b[1;32m    128\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _binary_auprc_compute(\n\u001b[1;32m    130\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    131\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_tasks,\n\u001b[1;32m    133\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/m3care/lib/python3.12/site-packages/torcheval/metrics/functional/classification/auprc.py:244\u001b[0m, in \u001b[0;36m_binary_auprc_compute\u001b[0;34m(input, target, num_tasks)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_binary_auprc_compute\u001b[39m(\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor, target: torch\u001b[38;5;241m.\u001b[39mTensor, num_tasks: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    241\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# for one task preserve the ndim of the input and target tensor\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_tasks \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         p, r, t \u001b[38;5;241m=\u001b[39m _compute_for_each_class(\u001b[38;5;28minput\u001b[39m, target, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _riemann_integral(r, p)\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/p19/p19_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "best_vloss = 1e6\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    rd_cls.train(True)\n",
    "    avg_loss, avg_auroc, avg_auprc = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    running_vloss, running_vauroc, running_vauprc = 0., 0., 0.\n",
    "    rd_cls.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dl):\n",
    "            ts_inp, times, mask, static_inp, labels = vdata\n",
    "            voutputs, reg_loss = rd_cls(ts_inp, times, mask, static_inp)\n",
    "            vloss = loss_fn(voutputs, labels, reg_loss)\n",
    "            # vloss = loss_fn(voutputs, labels)\n",
    "            bin_voutputs = voutputs.argmax(dim=-1)\n",
    "            bin_auroc_metric.update(bin_voutputs, labels)\n",
    "            bin_auprc_metric.update(bin_voutputs, labels)\n",
    "\n",
    "            running_vloss += vloss\n",
    "            running_vauroc += bin_auroc_metric.compute()\n",
    "            running_vauprc += bin_auprc_metric.compute()\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    avg_vauroc = running_vauroc / (i + 1)\n",
    "    avg_vauprc = running_vauprc / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation AUROC',\n",
    "                    { 'Training' : avg_auroc, 'Validation' : avg_vauroc },\n",
    "                    epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation AUPRC',\n",
    "                    { 'Training' : avg_auprc, 'Validation' : avg_vauprc },\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    writer.flush()\n",
    "\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(rd_cls.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
