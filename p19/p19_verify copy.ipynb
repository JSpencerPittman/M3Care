{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P19 Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torcheval.metrics import BinaryAUPRC, BinaryAUROC\n",
    "\n",
    "import wandb\n",
    "\n",
    "os.chdir('..')\n",
    "from raindrop.raindrop import Raindrop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_PATH = Path('./p19/data/processed_data')\n",
    "static_feat_names = np.load(PROCESSED_PATH / 'labels_demogr.npy')\n",
    "ts_feat_names = np.load(PROCESSED_PATH / 'labels_ts.npy')\n",
    "inputs = np.load(PROCESSED_PATH / 'PT_dict_list_6.npy', allow_pickle=True)\n",
    "labels = np.load(PROCESSED_PATH / 'arr_outcomes_6.npy').squeeze()\n",
    "\n",
    "ts_inputs = np.array([inp['arr'] for inp in inputs])[:, :, :, np.newaxis]\n",
    "static_inputs = np.array([inp['extended_static'] for inp in inputs])\n",
    "times = np.array([inp['time'] for inp in inputs]).squeeze()\n",
    "lengths = np.array([inp['length'] for inp in inputs])\n",
    "\n",
    "ts_inputs = torch.tensor(ts_inputs, dtype=torch.float32).to(device)\n",
    "static_inputs = torch.tensor(static_inputs, dtype=torch.float32).to(device)\n",
    "times = torch.tensor(times, dtype=torch.float32).to(device)\n",
    "lengths = torch.tensor(lengths).to(device)\n",
    "labels = torch.tensor(labels, dtype=torch.int64).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_samples = ts_inputs.shape[0]\n",
    "idxs = np.arange(num_samples)\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "train_idxs, val_idxs, test_idxs = idxs[:(s1 := int(num_samples*0.8))], idxs[s1: (s2 := int(num_samples*0.9))], idxs[s2:]\n",
    "\n",
    "train_ts_inp, val_ts_inp, test_ts_inp = ts_inputs[train_idxs], ts_inputs[val_idxs], ts_inputs[test_idxs]\n",
    "train_static_inp, val_static_inp, test_static_inp = static_inputs[train_idxs], static_inputs[val_idxs], static_inputs[test_idxs]\n",
    "train_times, val_times, test_times = times[train_idxs], times[val_idxs], times[test_idxs]\n",
    "train_lengths, val_lengths, test_lengths = lengths[train_idxs], lengths[val_idxs], lengths[test_idxs]\n",
    "train_lbls, val_lbls, test_lbls = labels[train_idxs], labels[val_idxs], labels[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P19 Summary\n",
      "Num Samples: 38803\n",
      "\tTrain: 31042\n",
      "\tVal: 3880\n",
      "\tTest: 3881\n",
      "Classes: 1626 4.19%\n",
      "\tTrain: 1626 4.19%\n",
      "\tVal: 1299 4.18%\n",
      "\tTest: 175 4.51%\n"
     ]
    }
   ],
   "source": [
    "print(\"P19 Summary\")\n",
    "print(f\"Num Samples: {num_samples}\\n\\tTrain: {train_idxs.shape[0]}\\n\\tVal: {val_idxs.shape[0]}\\n\\tTest: {test_idxs.shape[0]}\")\n",
    "\n",
    "pos = [(int(t := labels.sum()), 100 * t / labels.shape[0])] + \\\n",
    "      [(t := int(lbls.sum()), 100 * t / lbls.shape[0]) for lbls in (train_lbls, val_lbls, test_lbls)]\n",
    "print(f\"Classes: {pos[0][0]} {pos[0][1]:.2f}%\")\n",
    "for lbl, (t, p) in zip(['Train', 'Val', 'Test'], pos):\n",
    "    print(f\"\\t{lbl}: {t} {p:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor\n",
    "\n",
    "class P19Dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 ts_inp: Tensor,\n",
    "                 times: Tensor,\n",
    "                 lengths: Tensor,\n",
    "                 static_inp: Tensor,\n",
    "                 labels: Tensor,\n",
    "                 device: str):\n",
    "        self.ts_inp = ts_inp\n",
    "        self.times = times\n",
    "        self.lengths = lengths\n",
    "        self.static_inp = static_inp\n",
    "        self.labels = labels\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx: int | slice):\n",
    "        ts_inp = self.ts_inp[idx]\n",
    "\n",
    "        # Create mask\n",
    "        mask = torch.zeros(ts_inp.shape[:-1], dtype=bool, device=self.device)\n",
    "        if isinstance(idx, int):\n",
    "            mask[:self.lengths[idx]] = 1\n",
    "        else:\n",
    "            for idx, length in enumerate(self.lengths[idx]):\n",
    "                mask[idx, :length] = 1\n",
    "\n",
    "        return ts_inp, self.times[idx], mask, self.static_inp[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = P19Dataset(train_ts_inp, train_times, train_lengths, train_static_inp, train_lbls, device)\n",
    "val_ds = P19Dataset(val_ts_inp, val_times, val_lengths, val_static_inp, val_lbls, device)\n",
    "test_ds = P19Dataset(test_ts_inp, test_times, test_lengths, test_static_inp, test_lbls, device)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raindrop = Raindrop(num_sensors=34,\n",
    "                 obs_dim=1,\n",
    "                 obs_embed_dim=4,\n",
    "                 pe_emb_dim=16,\n",
    "                 timesteps=60,\n",
    "                 out_dim=128,\n",
    "                 num_heads=1,\n",
    "                 num_layers=1,\n",
    "                 inter_sensor_attn_dim=16,\n",
    "                 temporal_attn_dim=16,\n",
    "                 prune_rate=0.5,\n",
    "                 device=device)\n",
    "\n",
    "\n",
    "\n",
    "class RaindropClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 rd_model: Raindrop,\n",
    "                 static_dim: int,\n",
    "                 static_proj_dim: int,\n",
    "                 cls_hidden_dim: int,\n",
    "                 classes: int):\n",
    "        super().__init__()\n",
    "        self.static_dim = static_dim\n",
    "        self.static_proj_dim = static_proj_dim\n",
    "        self.cls_hidden_dim = cls_hidden_dim\n",
    "        self.classes = classes\n",
    "\n",
    "        self.rd_model = rd_model\n",
    "        self.static_proj = nn.Linear(static_dim, static_proj_dim)\n",
    "\n",
    "        rd_out_dim = self.rd_model.out_dim * self.rd_model.num_sensors\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(rd_out_dim + static_proj_dim, cls_hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(cls_hidden_dim, classes),\n",
    "            nn.LeakyReLU())\n",
    "\n",
    "\n",
    "    def forward(self, x_ts, times, mask, x_static):\n",
    "        ts_emb, reg_loss = self.rd_model(x_ts, times, mask)\n",
    "        ts_emb = ts_emb.view(ts_emb.shape[0], -1)\n",
    "\n",
    "        static_emb = F.leaky_relu(self.static_proj(x_static))\n",
    "\n",
    "        emb = torch.concat([ts_emb, static_emb], dim=-1) \n",
    "        return F.softmax(self.cls(emb), dim=-1), reg_loss\n",
    "    \n",
    "rd_cls = RaindropClassifier(raindrop,\n",
    "                            static_dim=6,\n",
    "                            static_proj_dim=34,\n",
    "                            cls_hidden_dim=128,\n",
    "                            classes=2).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "NUM_EPOCHS = 5\n",
    "LOSS_TRAIN_LOG_FREQ = 100\n",
    "\n",
    "optim = torch.optim.Adam(rd_cls.parameters(), lr=0.0001)\n",
    "\n",
    "bin_auroc_metric = BinaryAUROC()\n",
    "bin_auprc_metric = BinaryAUPRC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjspencerpittman\u001b[0m (\u001b[33mjspencerpittman-personal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Jason\\Work\\M3Care\\wandb\\run-20240921_130820-waevysq0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jspencerpittman-personal/m3care-raindrop/runs/waevysq0' target=\"_blank\">whole-totem-5</a></strong> to <a href='https://wandb.ai/jspencerpittman-personal/m3care-raindrop' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jspencerpittman-personal/m3care-raindrop' target=\"_blank\">https://wandb.ai/jspencerpittman-personal/m3care-raindrop</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jspencerpittman-personal/m3care-raindrop/runs/waevysq0' target=\"_blank\">https://wandb.ai/jspencerpittman-personal/m3care-raindrop/runs/waevysq0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.integration.torch.wandb_torch.TorchGraph at 0x1f665f78fa0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='m3care-raindrop')\n",
    "wandb.watch(rd_cls, log_freq=100, log_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss, running_auroc, running_auprc = 0., 0., 0.\n",
    "    last_loss, last_auroc, last_auprc = 0., 0., 0.\n",
    "\n",
    "    for i, data in enumerate(train_dl):\n",
    "        ts_inp, times, mask, static_inp, labels = data\n",
    "\n",
    "        optim.zero_grad()\n",
    "        outputs, reg_loss = rd_cls(ts_inp, times, mask, static_inp)\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)\n",
    "        bin_outputs = outputs.argmax(dim=-1)\n",
    "        bin_auroc_metric.update(bin_outputs, labels)\n",
    "        bin_auprc_metric.update(bin_outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_auroc += bin_auroc_metric.compute().item()\n",
    "        running_auprc += bin_auprc_metric.compute().item()\n",
    "\n",
    "        if i % LOSS_TRAIN_LOG_FREQ == LOSS_TRAIN_LOG_FREQ-1:\n",
    "            last_loss = running_loss / LOSS_TRAIN_LOG_FREQ\n",
    "            last_auroc = running_auroc / LOSS_TRAIN_LOG_FREQ\n",
    "            last_auprc = running_auprc / LOSS_TRAIN_LOG_FREQ\n",
    "    \n",
    "            print('  batch {} loss: {} AUROC: {} AUPRC {}'.format(i + 1, last_loss, last_auroc, last_auprc))\n",
    "            \n",
    "            # tb_x = epoch_index * len(train_dl) + i + 1\n",
    "            # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            # tb_writer.add_scalar('AUROC/train', last_auroc, tb_x)\n",
    "            # tb_writer.add_scalar('AUPRC/train', last_auprc, tb_x)\n",
    "\n",
    "            running_loss, running_auroc, running_auprc = 0., 0., 0.\n",
    "\n",
    "    return last_loss, last_auroc, last_auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 0.37960507154464723 AUROC: 0.492540183867226 AUPRC 0.04008268147706986\n",
      "  batch 200 loss: 0.3586616665124893 AUROC: 0.49875081354193485 AUPRC 0.04382984288036823\n",
      "  batch 300 loss: 0.3558755329251289 AUROC: 0.499264890150414 AUPRC 0.043859416842460634\n",
      "  batch 400 loss: 0.35348253816366193 AUROC: 0.4994888618052535 AUPRC 0.043052230067551135\n",
      "LOSS train 0.35348253816366193 valid 0.358272910118103\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.3528174668550491 AUROC: 0.49970799548749656 AUPRC 0.04215951714664698\n",
      "  batch 200 loss: 0.35296499460935593 AUROC: 0.49975407881903344 AUPRC 0.041594422459602355\n",
      "  batch 300 loss: 0.35905243813991544 AUROC: 0.49978447048028096 AUPRC 0.04170239489525557\n",
      "  batch 400 loss: 0.3559242370724678 AUROC: 0.4998071251745025 AUPRC 0.041961864978075025\n",
      "LOSS train 0.3559242370724678 valid 0.35824495553970337\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 0.35326502472162247 AUROC: 0.49984820855331547 AUPRC 0.0420757008716464\n",
      "  batch 200 loss: 0.3573267075419426 AUROC: 0.499860406483012 AUPRC 0.042087783329188826\n",
      "  batch 300 loss: 0.35779485940933226 AUROC: 0.49987025370976973 AUPRC 0.04224868968129158\n",
      "  batch 400 loss: 0.35091945946216585 AUROC: 0.4998793797089005 AUPRC 0.042209584787487986\n",
      "LOSS train 0.35091945946216585 valid 0.35839608311653137\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 0.35795018941164014 AUROC: 0.499896675496441 AUPRC 0.042310222052037716\n",
      "  batch 200 loss: 0.3542000287771225 AUROC: 0.49990236993442666 AUPRC 0.04234281934797764\n",
      "  batch 300 loss: 0.3545124018192291 AUROC: 0.4999077185262804 AUPRC 0.04227408353239298\n",
      "  batch 400 loss: 0.35544978886842726 AUROC: 0.4999125894316828 AUPRC 0.04218059845268726\n",
      "LOSS train 0.35544978886842726 valid 0.3585491478443146\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 0.35669963508844377 AUROC: 0.4999220666017118 AUPRC 0.04224760893732309\n",
      "  batch 200 loss: 0.3584183460474014 AUROC: 0.4999251545859821 AUPRC 0.04237233124673367\n",
      "  batch 300 loss: 0.3529495644569397 AUROC: 0.4999283179139621 AUPRC 0.04232767611742019\n",
      "  batch 400 loss: 0.3549807742238045 AUROC: 0.4999312487693943 AUPRC 0.0422743010148406\n",
      "LOSS train 0.3549807742238045 valid 0.3587026000022888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d6189560e14d7c8bbf10dd208f0896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.039 MB of 0.039 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training AUPRC</td><td>█▁▃▂▃</td></tr><tr><td>Training AUROC</td><td>▁▆▇██</td></tr><tr><td>Training Loss</td><td>▅█▁▇▇</td></tr><tr><td>Validation AUPRC</td><td>█▁▃▂▃</td></tr><tr><td>Validation AUROC</td><td>▁▅▇██</td></tr><tr><td>Validation Loss</td><td>▁▁▃▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training AUPRC</td><td>0.04227</td></tr><tr><td>Training AUROC</td><td>0.49993</td></tr><tr><td>Training Loss</td><td>0.35498</td></tr><tr><td>Validation AUPRC</td><td>0.04227</td></tr><tr><td>Validation AUROC</td><td>0.49994</td></tr><tr><td>Validation Loss</td><td>0.3587</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">whole-totem-5</strong> at: <a href='https://wandb.ai/jspencerpittman-personal/m3care-raindrop/runs/waevysq0' target=\"_blank\">https://wandb.ai/jspencerpittman-personal/m3care-raindrop/runs/waevysq0</a><br/> View project at: <a href='https://wandb.ai/jspencerpittman-personal/m3care-raindrop' target=\"_blank\">https://wandb.ai/jspencerpittman-personal/m3care-raindrop</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240921_130820-waevysq0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# writer = SummaryWriter('runs/p19/p19_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "best_vloss = 1e6\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    rd_cls.train(True)\n",
    "    avg_loss, avg_auroc, avg_auprc = train_one_epoch(epoch_number)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    running_vloss, running_vauroc, running_vauprc = 0., 0., 0.\n",
    "    rd_cls.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dl):\n",
    "            ts_inp, times, mask, static_inp, labels = vdata\n",
    "            voutputs, reg_loss = rd_cls(ts_inp, times, mask, static_inp)\n",
    "            vloss = loss_fn(voutputs, labels)\n",
    "            bin_voutputs = voutputs.argmax(dim=-1)\n",
    "            bin_auroc_metric.update(bin_voutputs, labels)\n",
    "            bin_auprc_metric.update(bin_voutputs, labels)\n",
    "\n",
    "            running_vloss += vloss\n",
    "            running_vauroc += bin_auroc_metric.compute()\n",
    "            running_vauprc += bin_auprc_metric.compute()\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    avg_vauroc = running_vauroc / (i + 1)\n",
    "    avg_vauprc = running_vauprc / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # writer.add_scalars('Training vs. Validation Loss',\n",
    "    #                 { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "    #                 epoch_number + 1)\n",
    "    # writer.add_scalars('Training vs. Validation AUROC',\n",
    "    #                 { 'Training' : avg_auroc, 'Validation' : avg_vauroc },\n",
    "    #                 epoch_number + 1)\n",
    "    # writer.add_scalars('Training vs. Validation AUPRC',\n",
    "    #                 { 'Training' : avg_auprc, 'Validation' : avg_vauprc },\n",
    "    #                 epoch_number + 1)\n",
    "    # writer.flush()\n",
    "    wandb.log({\n",
    "        'Training Loss': avg_loss,\n",
    "        'Training AUROC': avg_auroc,\n",
    "        'Training AUPRC': avg_auprc,\n",
    "        'Validation AUROC': avg_vauroc,\n",
    "        'Validation AUPRC': avg_auprc,\n",
    "        'Validation Loss': avg_vloss\n",
    "        })\n",
    "\n",
    "\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(rd_cls.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic3",
   "language": "python",
   "name": "mimic3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
