{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P19 Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torcheval.metrics import BinaryAUROC, BinaryAUPRC\n",
    "\n",
    "os.chdir('../..')\n",
    "from src.raindrop.raindrop import Raindrop\n",
    "from src.util.grad_track import GradientTracker, GradientFlowAnalyzer, pretty_flow\n",
    "from src.p19.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P19 Summary\n",
      "\tTotal samples 38803\n",
      "\t\tTraining: 31042\n",
      "\t\tValidation: 3880\n",
      "\t\tTesting: 3881\n",
      "\tClasses 4.19% positive\n",
      "\t\tTraining: 4.18%\n",
      "\t\tValidation: 4.51%\n",
      "\t\tTesting: 3.92%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ts_inputs, static_inputs, times, lengths, labels = \\\n",
    "    load_p19_data(Path('./data/p19/processed_data'), device)\n",
    "\n",
    "train_ds, val_ds, test_ds = split_p19_data(ts_inputs,\n",
    "                                           static_inputs,\n",
    "                                           times,\n",
    "                                           lengths,\n",
    "                                           labels,\n",
    "                                           device,\n",
    "                                           summary=True)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.raindrop.classifier import RaindropClassifier\n",
    "\n",
    "raindrop = Raindrop(num_sensors=34,\n",
    "                 obs_dim=1,\n",
    "                 obs_embed_dim=4,\n",
    "                 pe_emb_dim=16,\n",
    "                 timesteps=60,\n",
    "                 out_dim=128,\n",
    "                 num_heads=1,\n",
    "                 num_layers=2,\n",
    "                 inter_sensor_attn_dim=16,\n",
    "                 temporal_attn_dim=16,\n",
    "                 prune_rate=0.5,\n",
    "                 device=device)\n",
    "\n",
    "rd_cls = RaindropClassifier(raindrop,\n",
    "                            static_dim=6,\n",
    "                            static_proj_dim=34,\n",
    "                            cls_hidden_dim=128,\n",
    "                            classes=2).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RaindropLoss(nn.Module):\n",
    "    def __init__(self, reg_weight: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.reg_weight = reg_weight\n",
    "\n",
    "    def forward(self, predictions, targets, reg_loss):\n",
    "        ce_loss = self.ce_loss(predictions, targets)\n",
    "        reg_loss = reg_loss * self.reg_weight\n",
    "        return ce_loss + reg_loss\n",
    "    \n",
    "loss_fn = RaindropLoss(reg_weight=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "LOSS_TRAIN_LOG_FREQ = 100\n",
    "\n",
    "optim = torch.optim.Adam(rd_cls.parameters(), lr=0.0001)\n",
    "\n",
    "bin_auroc_metric = BinaryAUROC()\n",
    "bin_auprc_metric = BinaryAUPRC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = GradientTracker(rd_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss, running_auroc, running_auprc = 0., 0., 0.\n",
    "    last_loss, last_auroc, last_auprc = 0., 0., 0.\n",
    "\n",
    "    for i, data in enumerate(train_dl):\n",
    "        ts_inp, times, mask, static_inp, labels = data\n",
    "\n",
    "        optim.zero_grad()\n",
    "        outputs, reg_loss = rd_cls(ts_inp, times, mask, static_inp)\n",
    "        loss = loss_fn(outputs, labels, reg_loss)\n",
    "        bin_outputs = outputs.argmax(dim=-1)\n",
    "        bin_auroc_metric.update(bin_outputs, labels)\n",
    "        bin_auprc_metric.update(bin_outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_auroc += bin_auroc_metric.compute().item()\n",
    "        running_auprc += bin_auprc_metric.compute().item()\n",
    "\n",
    "        if i % LOSS_TRAIN_LOG_FREQ == LOSS_TRAIN_LOG_FREQ-1:\n",
    "            # return \n",
    "            last_loss = running_loss / LOSS_TRAIN_LOG_FREQ\n",
    "            last_auroc = running_auroc / LOSS_TRAIN_LOG_FREQ\n",
    "            last_auprc = running_auprc / LOSS_TRAIN_LOG_FREQ\n",
    "    \n",
    "            print('  batch {} loss: {} AUROC: {} AUPRC {}'.format(i + 1, last_loss, last_auroc, last_auprc))\n",
    "            \n",
    "            tb_x = epoch_index * len(train_dl) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            tb_writer.add_scalar('AUROC/train', last_auroc, tb_x)\n",
    "            tb_writer.add_scalar('AUPRC/train', last_auprc, tb_x)\n",
    "\n",
    "            running_loss, running_auroc, running_auprc = 0., 0., 0.\n",
    "\n",
    "    return last_loss, last_auroc, last_auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 0.6301598900556564 AUROC: 0.5106674200788046 AUPRC 0.04494955882430077\n",
      "  batch 200 loss: 0.593371514081955 AUROC: 0.5020044602005646 AUPRC 0.04307441234588623\n",
      "LOSS train 0.593371514081955 valid 0.5489270091056824\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.6118075355887413 AUROC: 0.5173173589109406 AUPRC 0.05362440627068281\n",
      "  batch 200 loss: 0.48340557277202606 AUROC: 0.5351300301330099 AUPRC 0.07612887054681777\n",
      "LOSS train 0.48340557277202606 valid 0.49864208698272705\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 0.5363694867491722 AUROC: 0.56133177033157 AUPRC 0.12042003445327282\n",
      "  batch 200 loss: 0.4741417223215103 AUROC: 0.572846008347649 AUPRC 0.1407277588546276\n",
      "LOSS train 0.4741417223215103 valid 0.468389630317688\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 0.44373813062906264 AUROC: 0.5817662795389955 AUPRC 0.1586796249449253\n",
      "  batch 200 loss: 0.4776214790344238 AUROC: 0.585659882647915 AUPRC 0.16850869342684746\n",
      "LOSS train 0.4776214790344238 valid 14295.8125\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 0.43320504486560824 AUROC: 0.5926030725878255 AUPRC 0.1859308059513569\n",
      "  batch 200 loss: 0.4238361892104149 AUROC: 0.5952322452778848 AUPRC 0.19302113831043244\n",
      "LOSS train 0.4238361892104149 valid 0.40535303950309753\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 0.39674607425928116 AUROC: 0.5981845975657503 AUPRC 0.20170315578579903\n",
      "  batch 200 loss: 0.3974489530920982 AUROC: 0.5996198331926785 AUPRC 0.20585052981972696\n",
      "LOSS train 0.3974489530920982 valid 0.39066818356513977\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 0.3778452116250992 AUROC: 0.6024050237977925 AUPRC 0.21356350302696228\n",
      "  batch 200 loss: 0.3740142813324928 AUROC: 0.6033582710731485 AUPRC 0.2165204481780529\n",
      "LOSS train 0.3740142813324928 valid 0.409647136926651\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 0.36731364637613295 AUROC: 0.6054444943388577 AUPRC 0.2223321282863617\n",
      "  batch 200 loss: 0.3613838508725166 AUROC: 0.6062187070596425 AUPRC 0.2247660391032696\n",
      "LOSS train 0.3613838508725166 valid 0.36089661717414856\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 0.3590892481803894 AUROC: 0.6080027240013581 AUPRC 0.2296632657945156\n",
      "  batch 200 loss: 0.35095977306365966 AUROC: 0.6086303117913503 AUPRC 0.2316049386560917\n",
      "LOSS train 0.35095977306365966 valid 0.3538166284561157\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 0.3508063557744026 AUROC: 0.6096735125236077 AUPRC 0.2342528547346592\n",
      "  batch 200 loss: 0.34890989542007445 AUROC: 0.6102323611861705 AUPRC 0.23572596281766892\n",
      "LOSS train 0.34890989542007445 valid 2.1440277099609375\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 0.34737082302570343 AUROC: 0.6115658510862247 AUPRC 0.2392891502380371\n",
      "  batch 200 loss: 0.34468604445457457 AUROC: 0.6124012051086285 AUPRC 0.24140464186668395\n",
      "LOSS train 0.34468604445457457 valid 0.34856152534484863\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 0.3454700431227684 AUROC: 0.612800636037635 AUPRC 0.24297413662075995\n",
      "  batch 200 loss: 0.34688645631074905 AUROC: 0.613375197608758 AUPRC 0.24426377236843108\n",
      "LOSS train 0.34688645631074905 valid 0.3476715385913849\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 0.34526567041873935 AUROC: 0.6136374208993302 AUPRC 0.24548182457685472\n",
      "  batch 200 loss: 0.346635282933712 AUROC: 0.613779113151294 AUPRC 0.24611857637763024\n",
      "LOSS train 0.346635282933712 valid 0.34702935814857483\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 0.3465227457880974 AUROC: 0.614123456558486 AUPRC 0.24745432823896407\n",
      "  batch 200 loss: 0.3433793073892593 AUROC: 0.6144714953721719 AUPRC 0.24842395067214965\n",
      "LOSS train 0.3433793073892593 valid 0.3474479913711548\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 0.3467813524603844 AUROC: 0.6148955271794719 AUPRC 0.2498053441941738\n",
      "  batch 200 loss: 0.34213920891284944 AUROC: 0.6151918167970636 AUPRC 0.25055942639708517\n",
      "LOSS train 0.34213920891284944 valid 0.34656888246536255\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 0.3447870522737503 AUROC: 0.6155220771052577 AUPRC 0.25168914258480074\n",
      "  batch 200 loss: 0.34649110823869705 AUROC: 0.6155841321227232 AUPRC 0.25207112669944765\n",
      "LOSS train 0.34649110823869705 valid 4.824400901794434\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 0.3453045830130577 AUROC: 0.615899705576991 AUPRC 0.25310848325490953\n",
      "  batch 200 loss: 0.3435050445795059 AUROC: 0.616042511785929 AUPRC 0.25359560251235963\n",
      "LOSS train 0.3435050445795059 valid 0.34717562794685364\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 0.346151762008667 AUROC: 0.6162924754735427 AUPRC 0.25449610233306885\n",
      "  batch 200 loss: 0.3438816398382187 AUROC: 0.6162578727592384 AUPRC 0.2546101263165474\n",
      "LOSS train 0.3438816398382187 valid 0.3472006320953369\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 0.34419096559286116 AUROC: 0.6165387258762575 AUPRC 0.2555126616358757\n",
      "  batch 200 loss: 0.3463756990432739 AUROC: 0.6168038361474028 AUPRC 0.2561786687374115\n",
      "LOSS train 0.3463756990432739 valid 0.3458709716796875\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 0.3442659232020378 AUROC: 0.6168582587695949 AUPRC 0.2565801110863686\n",
      "  batch 200 loss: 0.3456693759560585 AUROC: 0.6169001819578712 AUPRC 0.2568295460939407\n",
      "LOSS train 0.3456693759560585 valid 0.437195748090744\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/p19/p19_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "best_vloss = 1e6\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    rd_cls.train(True)\n",
    "    avg_loss, avg_auroc, avg_auprc = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    running_vloss, running_vauroc, running_vauprc = 0., 0., 0.\n",
    "    rd_cls.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dl):\n",
    "            ts_inp, times, mask, static_inp, labels = vdata\n",
    "            voutputs, reg_loss = rd_cls(ts_inp, times, mask, static_inp)\n",
    "            vloss = loss_fn(voutputs, labels, reg_loss)\n",
    "            bin_voutputs = voutputs.argmax(dim=-1)\n",
    "            bin_auroc_metric.update(bin_voutputs, labels)\n",
    "            bin_auprc_metric.update(bin_voutputs, labels)\n",
    "\n",
    "            running_vloss += vloss\n",
    "            running_vauroc += bin_auroc_metric.compute()\n",
    "            running_vauprc += bin_auprc_metric.compute()\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    avg_vauroc = running_vauroc / (i + 1)\n",
    "    avg_vauprc = running_vauprc / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation AUROC',\n",
    "                    { 'Training' : avg_auroc, 'Validation' : avg_vauroc },\n",
    "                    epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation AUPRC',\n",
    "                    { 'Training' : avg_auprc, 'Validation' : avg_vauprc },\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    writer.flush()\n",
    "\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = './models/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(rd_cls.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rd_model.sensor_embed_ln(O->I): 5.911503e-03 -> 2.442401e-03\n",
      "rd_model.sensor_embed_map(O->I): 1.753780e-03 -> 1.001158e-03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gfa = GradientFlowAnalyzer(gt)\n",
    "\n",
    "flow = [\n",
    "    'rd_model.sensor_embed_ln',\n",
    "    'rd_model.sensor_embed_map',\n",
    "]\n",
    "\n",
    "print(pretty_flow(gfa.flow(flow, 0)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
