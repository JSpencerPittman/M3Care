{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P19 Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torcheval.metrics import BinaryAUROC, BinaryAUPRC\n",
    "\n",
    "os.chdir('..')\n",
    "from raindrop.raindrop import Raindrop\n",
    "from util.grad_track import GradientTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_PATH = Path('./p19/data/processed_data')\n",
    "static_feat_names = np.load(PROCESSED_PATH / 'labels_demogr.npy')\n",
    "ts_feat_names = np.load(PROCESSED_PATH / 'labels_ts.npy')\n",
    "inputs = np.load(PROCESSED_PATH / 'PT_dict_list_6.npy', allow_pickle=True)\n",
    "labels = np.load(PROCESSED_PATH / 'arr_outcomes_6.npy').squeeze()\n",
    "\n",
    "ts_inputs = np.array([inp['arr'] for inp in inputs])[:, :, :, np.newaxis]\n",
    "static_inputs = np.array([inp['extended_static'] for inp in inputs])\n",
    "times = np.array([inp['time'] for inp in inputs]).squeeze()\n",
    "lengths = np.array([inp['length'] for inp in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(ts_inputs, static_inputs, times, lengths, labels):\n",
    "    neg, pos = ~labels.astype(bool), labels.astype(bool)\n",
    "    num_neg, num_pos = sum(neg), sum(pos)\n",
    "    fill_to = num_neg\n",
    "    \n",
    "    def oversample_array(array):\n",
    "        arr_neg, arr_pos = array[neg], array[pos]\n",
    "        \n",
    "        arr_pos = np.repeat(arr_pos, fill_to // num_pos, axis=0)\n",
    "        if (rem := fill_to % num_pos):\n",
    "            arr_pos = np.concatenate([arr_pos, arr_pos[:rem]], axis=0)\n",
    "\n",
    "        return np.concatenate([arr_pos, arr_neg], axis=0)\n",
    "    \n",
    "    ts_inputs = oversample_array(ts_inputs)\n",
    "    static_inputs = oversample_array(static_inputs)\n",
    "    times = oversample_array(times)\n",
    "    lengths = oversample_array(lengths)\n",
    "    labels = oversample_array(labels)\n",
    "    \n",
    "    return ts_inputs, static_inputs, times, lengths, labels\n",
    "    \n",
    "ts_inputs, static_inputs, times, lengths, labels = oversample(ts_inputs, static_inputs, times, lengths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_inputs = torch.tensor(ts_inputs, dtype=torch.float32).to(device)\n",
    "static_inputs = torch.tensor(static_inputs, dtype=torch.float32).to(device)\n",
    "times = torch.tensor(times, dtype=torch.float32).to(device)\n",
    "lengths = torch.tensor(lengths).to(device)\n",
    "labels = torch.tensor(labels, dtype=torch.int64).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_samples = ts_inputs.shape[0]\n",
    "idxs = np.arange(num_samples)\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "train_idxs, val_idxs, test_idxs = idxs[:(s1 := int(num_samples*0.8))], idxs[s1: (s2 := int(num_samples*0.9))], idxs[s2:]\n",
    "\n",
    "train_ts_inp, val_ts_inp, test_ts_inp = ts_inputs[train_idxs], ts_inputs[val_idxs], ts_inputs[test_idxs]\n",
    "train_static_inp, val_static_inp, test_static_inp = static_inputs[train_idxs], static_inputs[val_idxs], static_inputs[test_idxs]\n",
    "train_times, val_times, test_times = times[train_idxs], times[val_idxs], times[test_idxs]\n",
    "train_lengths, val_lengths, test_lengths = lengths[train_idxs], lengths[val_idxs], lengths[test_idxs]\n",
    "train_lbls, val_lbls, test_lbls = labels[train_idxs], labels[val_idxs], labels[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P19 Summary\n",
      "Num Samples: 74354\n",
      "\tTrain: 59483\n",
      "\tVal: 7435\n",
      "\tTest: 7436\n",
      "Classes: 37177 50.00%\n",
      "\tTrain: 37177 50.00%\n",
      "\tVal: 29659 49.86%\n",
      "\tTest: 3684 49.55%\n"
     ]
    }
   ],
   "source": [
    "print(\"P19 Summary\")\n",
    "print(f\"Num Samples: {num_samples}\\n\\tTrain: {train_idxs.shape[0]}\\n\\tVal: {val_idxs.shape[0]}\\n\\tTest: {test_idxs.shape[0]}\")\n",
    "\n",
    "pos = [(int(t := labels.sum()), 100 * t / labels.shape[0])] + \\\n",
    "      [(t := int(lbls.sum()), 100 * t / lbls.shape[0]) for lbls in (train_lbls, val_lbls, test_lbls)]\n",
    "print(f\"Classes: {pos[0][0]} {pos[0][1]:.2f}%\")\n",
    "for lbl, (t, p) in zip(['Train', 'Val', 'Test'], pos):\n",
    "    print(f\"\\t{lbl}: {t} {p:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor\n",
    "\n",
    "class P19Dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 ts_inp: Tensor,\n",
    "                 times: Tensor,\n",
    "                 lengths: Tensor,\n",
    "                 static_inp: Tensor,\n",
    "                 labels: Tensor,\n",
    "                 device: str):\n",
    "        self.ts_inp = ts_inp\n",
    "        self.times = times\n",
    "        self.lengths = lengths\n",
    "        self.static_inp = static_inp\n",
    "        self.labels = labels\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, loc: int | slice):\n",
    "        ts_inp = self.ts_inp[loc]\n",
    "\n",
    "        # Create mask\n",
    "        mask = torch.zeros(ts_inp.shape[:-1], dtype=bool, device=self.device)\n",
    "        if isinstance(loc, int):\n",
    "            mask[:self.lengths[loc]] = 1\n",
    "        else:\n",
    "            for idx, length in enumerate(self.lengths[loc]):\n",
    "                mask[idx, :length] = 1\n",
    "\n",
    "        return ts_inp, self.times[loc], mask, self.static_inp[loc], self.labels[loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = P19Dataset(train_ts_inp, train_times, train_lengths, train_static_inp, train_lbls, device)\n",
    "val_ds = P19Dataset(val_ts_inp, val_times, val_lengths, val_static_inp, val_lbls, device)\n",
    "test_ds = P19Dataset(test_ts_inp, test_times, test_lengths, test_static_inp, test_lbls, device)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=128, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raindrop = Raindrop(num_sensors=34,\n",
    "                 obs_dim=1,\n",
    "                 obs_embed_dim=4,\n",
    "                 pe_emb_dim=16,\n",
    "                 timesteps=60,\n",
    "                 out_dim=128,\n",
    "                 num_heads=1,\n",
    "                 num_layers=1,\n",
    "                 inter_sensor_attn_dim=16,\n",
    "                 temporal_attn_dim=16,\n",
    "                 prune_rate=0.5,\n",
    "                 device=device)\n",
    "\n",
    "\n",
    "\n",
    "class RaindropClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 rd_model: Raindrop,\n",
    "                 static_dim: int,\n",
    "                 static_proj_dim: int,\n",
    "                 cls_hidden_dim: int,\n",
    "                 classes: int):\n",
    "        super().__init__()\n",
    "        self.static_dim = static_dim\n",
    "        self.static_proj_dim = static_proj_dim\n",
    "        self.cls_hidden_dim = cls_hidden_dim\n",
    "        self.classes = classes\n",
    "\n",
    "        self.rd_model = rd_model\n",
    "        self.static_proj = nn.Linear(static_dim, static_proj_dim)\n",
    "\n",
    "        rd_out_dim = self.rd_model.out_dim * self.rd_model.num_sensors\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(rd_out_dim + static_proj_dim, cls_hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(cls_hidden_dim, classes),\n",
    "            nn.LeakyReLU())\n",
    "\n",
    "\n",
    "    def forward(self, x_ts, times, mask, x_static):\n",
    "        ts_emb, reg_loss = self.rd_model(x_ts, times, mask)\n",
    "        ts_emb = ts_emb.view(ts_emb.shape[0], -1)\n",
    "\n",
    "        static_emb = F.leaky_relu(self.static_proj(x_static))\n",
    "\n",
    "        emb = torch.concat([ts_emb, static_emb], dim=-1) \n",
    "        return F.softmax(self.cls(emb), dim=-1), reg_loss\n",
    "    \n",
    "rd_cls = RaindropClassifier(raindrop,\n",
    "                            static_dim=6,\n",
    "                            static_proj_dim=34,\n",
    "                            cls_hidden_dim=128,\n",
    "                            classes=2).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RaindropLoss(nn.Module):\n",
    "    def __init__(self, reg_weight: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.reg_weight = reg_weight\n",
    "\n",
    "    def forward(self, predictions, targets, reg_loss):\n",
    "        ce_loss = self.ce_loss(predictions, targets)\n",
    "        # reg_loss *= self.reg_weight\n",
    "        # return ce_loss + reg_loss\n",
    "        return ce_loss\n",
    "    \n",
    "loss_fn = RaindropLoss(reg_weight=0.02)\n",
    "# loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "LOSS_TRAIN_LOG_FREQ = 100\n",
    "\n",
    "optim = torch.optim.Adam(rd_cls.parameters(), lr=0.0001)\n",
    "\n",
    "bin_auroc_metric = BinaryAUROC()\n",
    "bin_auprc_metric = BinaryAUPRC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt = GradientTracker(rd_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss, running_auroc, running_auprc = 0., 0., 0.\n",
    "    last_loss, last_auroc, last_auprc = 0., 0., 0.\n",
    "\n",
    "    for i, data in enumerate(train_dl):\n",
    "        ts_inp, times, mask, static_inp, labels = data\n",
    "\n",
    "        optim.zero_grad()\n",
    "        outputs, reg_loss = rd_cls(ts_inp, times, mask, static_inp)\n",
    "        loss = loss_fn(outputs, labels, reg_loss)\n",
    "        # loss = loss_fn(outputs, labels)\n",
    "        bin_outputs = outputs.argmax(dim=-1)\n",
    "        bin_auroc_metric.update(bin_outputs, labels)\n",
    "        bin_auprc_metric.update(bin_outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_auroc += bin_auroc_metric.compute().item()\n",
    "        running_auprc += bin_auprc_metric.compute().item()\n",
    "\n",
    "        if i % LOSS_TRAIN_LOG_FREQ == LOSS_TRAIN_LOG_FREQ-1:\n",
    "            # return \n",
    "            last_loss = running_loss / LOSS_TRAIN_LOG_FREQ\n",
    "            last_auroc = running_auroc / LOSS_TRAIN_LOG_FREQ\n",
    "            last_auprc = running_auprc / LOSS_TRAIN_LOG_FREQ\n",
    "    \n",
    "            print('  batch {} loss: {} AUROC: {} AUPRC {}'.format(i + 1, last_loss, last_auroc, last_auprc))\n",
    "            \n",
    "            tb_x = epoch_index * len(train_dl) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            tb_writer.add_scalar('AUROC/train', last_auroc, tb_x)\n",
    "            tb_writer.add_scalar('AUPRC/train', last_auprc, tb_x)\n",
    "\n",
    "            running_loss, running_auroc, running_auprc = 0., 0., 0.\n",
    "\n",
    "    return last_loss, last_auroc, last_auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 0.5808805045485497 AUROC: 0.6955615089275227 AUPRC 0.6407318341732026\n",
      "  batch 200 loss: 0.581168327331543 AUROC: 0.7135721607385856 AUPRC 0.6655090445280075\n",
      "  batch 300 loss: 0.5790930593013763 AUROC: 0.7144236927343147 AUPRC 0.6677868002653122\n",
      "  batch 400 loss: 0.5700709235668182 AUROC: 0.7153809397753207 AUPRC 0.6677020889520645\n",
      "LOSS train 0.5700709235668182 valid 0.5663967728614807\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.5678902086615563 AUROC: 0.7203547295172069 AUPRC 0.672495938539505\n",
      "  batch 200 loss: 0.5655494064092637 AUROC: 0.7213465432113333 AUPRC 0.6737380599975586\n",
      "  batch 300 loss: 0.5533120912313462 AUROC: 0.7237033848340998 AUPRC 0.6762548834085464\n",
      "  batch 400 loss: 0.5539217674732209 AUROC: 0.7260796801748255 AUPRC 0.6785876822471618\n",
      "LOSS train 0.5539217674732209 valid 0.5506409406661987\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 0.5466507241129875 AUROC: 0.7304866750830736 AUPRC 0.6829441636800766\n",
      "  batch 200 loss: 0.5446849721670151 AUROC: 0.7321095898429532 AUPRC 0.6842324149608612\n",
      "  batch 300 loss: 0.5408325821161271 AUROC: 0.7339368844087979 AUPRC 0.6859531301259995\n",
      "  batch 400 loss: 0.5405173432826996 AUROC: 0.7356843940882634 AUPRC 0.6876439672708511\n",
      "LOSS train 0.5405173432826996 valid 0.531164824962616\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 0.5290314555168152 AUROC: 0.7401660445687179 AUPRC 0.691965462565422\n",
      "  batch 200 loss: 0.5279580101370811 AUROC: 0.742105563203129 AUPRC 0.6938152545690537\n",
      "  batch 300 loss: 0.5249154618382454 AUROC: 0.7440844871448518 AUPRC 0.6957757896184922\n",
      "  batch 400 loss: 0.5238661029934883 AUROC: 0.7458652965123319 AUPRC 0.6974725836515426\n",
      "LOSS train 0.5238661029934883 valid 0.5094165205955505\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 0.5148515465855599 AUROC: 0.7502330750627391 AUPRC 0.7017299252748489\n",
      "  batch 200 loss: 0.5128376165032387 AUROC: 0.7519292236933857 AUPRC 0.7033269113302231\n",
      "  batch 300 loss: 0.5130033549666405 AUROC: 0.753710581643347 AUPRC 0.7050073355436325\n",
      "  batch 400 loss: 0.5131825217604638 AUROC: 0.7552394131582788 AUPRC 0.7064804852008819\n",
      "LOSS train 0.5131825217604638 valid 0.508483350276947\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 0.509161402285099 AUROC: 0.7584705593669093 AUPRC 0.709779634475708\n",
      "  batch 200 loss: 0.5040426307916641 AUROC: 0.7599255587868773 AUPRC 0.7111518055200576\n",
      "  batch 300 loss: 0.4997392749786377 AUROC: 0.7613938652908354 AUPRC 0.7124856722354889\n",
      "  batch 400 loss: 0.5027635252475738 AUROC: 0.7628531273448453 AUPRC 0.7139150297641754\n",
      "LOSS train 0.5027635252475738 valid 0.498788982629776\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 0.49585104614496234 AUROC: 0.7663931737239527 AUPRC 0.7176868414878845\n",
      "  batch 200 loss: 0.4975742360949516 AUROC: 0.76771622195464 AUPRC 0.7189816725254059\n",
      "  batch 300 loss: 0.49610073775053026 AUROC: 0.7690069090088391 AUPRC 0.7202602845430374\n",
      "  batch 400 loss: 0.49219512969255447 AUROC: 0.7702157373696029 AUPRC 0.7214681231975555\n",
      "LOSS train 0.49219512969255447 valid 0.49109578132629395\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 0.48856403470039367 AUROC: 0.7731112427744393 AUPRC 0.7243072950839996\n",
      "  batch 200 loss: 0.4848319354653359 AUROC: 0.7743762766055734 AUPRC 0.7255899780988693\n",
      "  batch 300 loss: 0.485757040977478 AUROC: 0.7756217950490059 AUPRC 0.7268504387140274\n",
      "  batch 400 loss: 0.48580193996429444 AUROC: 0.7768001892978517 AUPRC 0.7279763752222062\n",
      "LOSS train 0.48580193996429444 valid 0.48728880286216736\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 0.4832912945747376 AUROC: 0.779207230894707 AUPRC 0.7305069583654403\n",
      "  batch 200 loss: 0.4767598056793213 AUROC: 0.780409702515576 AUPRC 0.7317218297719955\n",
      "  batch 300 loss: 0.47959674030542376 AUROC: 0.7816069808356214 AUPRC 0.7328983646631241\n",
      "  batch 400 loss: 0.4797002473473549 AUROC: 0.7827479165877409 AUPRC 0.7341111850738525\n",
      "LOSS train 0.4797002473473549 valid 0.4838363826274872\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 0.4762025395035744 AUROC: 0.7849080717101109 AUPRC 0.7362937915325165\n",
      "  batch 200 loss: 0.4730591291189194 AUROC: 0.7859854921879248 AUPRC 0.7374029159545898\n",
      "  batch 300 loss: 0.47440407395362855 AUROC: 0.7870315275191875 AUPRC 0.7385717672109604\n",
      "  batch 400 loss: 0.4768123897910118 AUROC: 0.7879999802698282 AUPRC 0.739547056555748\n",
      "LOSS train 0.4768123897910118 valid 0.4731315076351166\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 0.46861400485038757 AUROC: 0.7900793936934933 AUPRC 0.7416174727678299\n",
      "  batch 200 loss: 0.4693718335032463 AUROC: 0.7910503466306739 AUPRC 0.7426361185312271\n",
      "  batch 300 loss: 0.4685497909784317 AUROC: 0.7919710435169032 AUPRC 0.7436205530166626\n",
      "  batch 400 loss: 0.469068184196949 AUROC: 0.7928717480320916 AUPRC 0.7445812511444092\n",
      "LOSS train 0.469068184196949 valid 0.46967488527297974\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 0.4688485333323479 AUROC: 0.7947133308607336 AUPRC 0.7463310098648072\n",
      "  batch 200 loss: 0.4603581875562668 AUROC: 0.7956031060652222 AUPRC 0.7472532641887665\n",
      "  batch 300 loss: 0.4631116408109665 AUROC: 0.7965270448762948 AUPRC 0.7482413899898529\n",
      "  batch 400 loss: 0.46434983402490615 AUROC: 0.7973359463551162 AUPRC 0.7491024672985077\n",
      "LOSS train 0.46434983402490615 valid 0.46704256534576416\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 0.45710535734891894 AUROC: 0.799080781487559 AUPRC 0.7509125882387161\n",
      "  batch 200 loss: 0.4611141249537468 AUROC: 0.7999404797447934 AUPRC 0.7518410444259643\n",
      "  batch 300 loss: 0.455909221470356 AUROC: 0.8008017870475469 AUPRC 0.7527977043390274\n",
      "  batch 400 loss: 0.4614339315891266 AUROC: 0.8016223719147413 AUPRC 0.7536803239583969\n",
      "LOSS train 0.4614339315891266 valid 0.45949792861938477\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 0.4566315150260925 AUROC: 0.8033390200643439 AUPRC 0.7555470907688141\n",
      "  batch 200 loss: 0.452198341190815 AUROC: 0.804157047034108 AUPRC 0.7564089286327362\n",
      "  batch 300 loss: 0.45209402710199353 AUROC: 0.8049730483096318 AUPRC 0.7572905170917511\n",
      "  batch 400 loss: 0.45002071976661684 AUROC: 0.8057901347128965 AUPRC 0.7581398510932922\n",
      "LOSS train 0.45002071976661684 valid 0.46616825461387634\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 0.4509613087773323 AUROC: 0.8073758177693072 AUPRC 0.7598221755027771\n",
      "  batch 200 loss: 0.44675908237695694 AUROC: 0.8081502619901745 AUPRC 0.7606457477807999\n",
      "  batch 300 loss: 0.44869706422090533 AUROC: 0.8088911498931034 AUPRC 0.7614197939634323\n",
      "  batch 400 loss: 0.45141675412654875 AUROC: 0.8096200028765326 AUPRC 0.7622139430046082\n",
      "LOSS train 0.45141675412654875 valid 0.45653411746025085\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 0.44890823513269423 AUROC: 0.8111237213374187 AUPRC 0.7638021594285965\n",
      "  batch 200 loss: 0.44751483768224715 AUROC: 0.8117855281419992 AUPRC 0.7645038515329361\n",
      "  batch 300 loss: 0.43937949031591417 AUROC: 0.8125158997846647 AUPRC 0.7652768671512604\n",
      "  batch 400 loss: 0.4470851069688797 AUROC: 0.8132377238253055 AUPRC 0.7660940164327621\n",
      "LOSS train 0.4470851069688797 valid 0.45336025953292847\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 0.44586636185646056 AUROC: 0.814696559276487 AUPRC 0.7676323664188385\n",
      "  batch 200 loss: 0.4370592260360718 AUROC: 0.8153955838555628 AUPRC 0.7684051829576493\n",
      "  batch 300 loss: 0.4380438143014908 AUROC: 0.8161218201501541 AUPRC 0.7692219001054764\n",
      "  batch 400 loss: 0.4417744055390358 AUROC: 0.8167503839394675 AUPRC 0.7699127823114396\n",
      "LOSS train 0.4417744055390358 valid 0.4433951675891876\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 0.43733890682458876 AUROC: 0.818190414133721 AUPRC 0.7714439803361892\n",
      "  batch 200 loss: 0.4368525639176369 AUROC: 0.8188713313390232 AUPRC 0.7722026807069778\n",
      "  batch 300 loss: 0.43940586745738985 AUROC: 0.8195007318196226 AUPRC 0.7729065895080567\n",
      "  batch 400 loss: 0.43478976041078565 AUROC: 0.8201391098026657 AUPRC 0.7736313891410828\n",
      "LOSS train 0.43478976041078565 valid 0.43255773186683655\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 0.4397368061542511 AUROC: 0.8215271949788998 AUPRC 0.7751587891578674\n",
      "  batch 200 loss: 0.43515697032213213 AUROC: 0.8221294777958811 AUPRC 0.7758787530660629\n",
      "  batch 300 loss: 0.4336441460251808 AUROC: 0.8227393829640401 AUPRC 0.7765486466884614\n",
      "  batch 400 loss: 0.43133694052696225 AUROC: 0.8233549589657663 AUPRC 0.7772229075431824\n",
      "LOSS train 0.43133694052696225 valid 0.451037734746933\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 0.42984933376312257 AUROC: 0.8246253437418646 AUPRC 0.7785503113269806\n",
      "  batch 200 loss: 0.4309730103611946 AUROC: 0.8252193481082497 AUPRC 0.7792080932855606\n",
      "  batch 300 loss: 0.4255416291952133 AUROC: 0.8258294564829153 AUPRC 0.7798855650424957\n",
      "  batch 400 loss: 0.4313585475087166 AUROC: 0.8264377827070791 AUPRC 0.7805718797445297\n",
      "LOSS train 0.4313585475087166 valid 0.43619900941848755\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/p19/p19_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "best_vloss = 1e6\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    rd_cls.train(True)\n",
    "    avg_loss, avg_auroc, avg_auprc = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    running_vloss, running_vauroc, running_vauprc = 0., 0., 0.\n",
    "    rd_cls.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dl):\n",
    "            ts_inp, times, mask, static_inp, labels = vdata\n",
    "            voutputs, reg_loss = rd_cls(ts_inp, times, mask, static_inp)\n",
    "            vloss = loss_fn(voutputs, labels, reg_loss)\n",
    "            # vloss = loss_fn(voutputs, labels)\n",
    "            bin_voutputs = voutputs.argmax(dim=-1)\n",
    "            bin_auroc_metric.update(bin_voutputs, labels)\n",
    "            bin_auprc_metric.update(bin_voutputs, labels)\n",
    "\n",
    "            running_vloss += vloss\n",
    "            running_vauroc += bin_auroc_metric.compute()\n",
    "            running_vauprc += bin_auprc_metric.compute()\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    avg_vauroc = running_vauroc / (i + 1)\n",
    "    avg_vauprc = running_vauprc / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation AUROC',\n",
    "                    { 'Training' : avg_auroc, 'Validation' : avg_vauroc },\n",
    "                    epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation AUPRC',\n",
    "                    { 'Training' : avg_auprc, 'Validation' : avg_vauprc },\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    writer.flush()\n",
    "\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(rd_cls.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic3",
   "language": "python",
   "name": "mimic3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
