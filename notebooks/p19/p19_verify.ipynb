{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P19 Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torcheval.metrics import BinaryAUROC, BinaryAUPRC\n",
    "\n",
    "os.chdir('../..')\n",
    "from src.raindrop.raindrop import Raindrop\n",
    "from src.util.grad_track import GradientTracker, GradientFlowAnalyzer, pretty_flow\n",
    "from src.p19.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P19 Summary\n",
      "\tTotal samples 38803\n",
      "\t\tTraining: 31042\n",
      "\t\tValidation: 3880\n",
      "\t\tTesting: 3881\n",
      "\tClasses 4.19% positive\n",
      "\t\tTraining: 4.18%\n",
      "\t\tValidation: 4.51%\n",
      "\t\tTesting: 3.92%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ts_inputs, static_inputs, times, lengths, labels = \\\n",
    "    load_p19_data(Path('./data/p19/processed_data'), device)\n",
    "\n",
    "train_ds, val_ds, test_ds = split_p19_data(ts_inputs,\n",
    "                                           static_inputs,\n",
    "                                           times,\n",
    "                                           lengths,\n",
    "                                           labels,\n",
    "                                           device,\n",
    "                                           summary=True)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.raindrop.classifier import RaindropClassifier\n",
    "\n",
    "raindrop = Raindrop(num_sensors=34,\n",
    "                 obs_dim=1,\n",
    "                 obs_embed_dim=4,\n",
    "                 pe_emb_dim=16,\n",
    "                 timesteps=60,\n",
    "                 out_dim=128,\n",
    "                 num_heads=1,\n",
    "                 num_layers=1,\n",
    "                 inter_sensor_attn_dim=16,\n",
    "                 temporal_attn_dim=16,\n",
    "                 prune_rate=0.5,\n",
    "                 device=device)\n",
    "\n",
    "rd_cls = RaindropClassifier(raindrop,\n",
    "                            static_dim=6,\n",
    "                            static_proj_dim=34,\n",
    "                            cls_hidden_dim=128,\n",
    "                            classes=2).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RaindropLoss(nn.Module):\n",
    "    def __init__(self, reg_weight: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.reg_weight = reg_weight\n",
    "\n",
    "    def forward(self, predictions, targets, reg_loss):\n",
    "        ce_loss = self.ce_loss(predictions, targets)\n",
    "        reg_loss *= self.reg_weight\n",
    "        return ce_loss + reg_loss\n",
    "    \n",
    "loss_fn = RaindropLoss(reg_weight=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "LOSS_TRAIN_LOG_FREQ = 100\n",
    "\n",
    "optim = torch.optim.Adam(rd_cls.parameters(), lr=0.0001)\n",
    "\n",
    "bin_auroc_metric = BinaryAUROC()\n",
    "bin_auprc_metric = BinaryAUPRC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = GradientTracker(rd_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss, running_auroc, running_auprc = 0., 0., 0.\n",
    "    last_loss, last_auroc, last_auprc = 0., 0., 0.\n",
    "\n",
    "    for i, data in enumerate(train_dl):\n",
    "        ts_inp, times, mask, static_inp, labels = data\n",
    "\n",
    "        optim.zero_grad()\n",
    "        outputs, reg_loss = rd_cls(ts_inp, times, mask, static_inp)\n",
    "        loss = loss_fn(outputs, labels, reg_loss)\n",
    "        # loss = loss_fn(outputs, labels)\n",
    "        bin_outputs = outputs.argmax(dim=-1)\n",
    "        bin_auroc_metric.update(bin_outputs, labels)\n",
    "        bin_auprc_metric.update(bin_outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_auroc += bin_auroc_metric.compute().item()\n",
    "        running_auprc += bin_auprc_metric.compute().item()\n",
    "\n",
    "        if i % LOSS_TRAIN_LOG_FREQ == LOSS_TRAIN_LOG_FREQ-1:\n",
    "            # return \n",
    "            last_loss = running_loss / LOSS_TRAIN_LOG_FREQ\n",
    "            last_auroc = running_auroc / LOSS_TRAIN_LOG_FREQ\n",
    "            last_auprc = running_auprc / LOSS_TRAIN_LOG_FREQ\n",
    "    \n",
    "            print('  batch {} loss: {} AUROC: {} AUPRC {}'.format(i + 1, last_loss, last_auroc, last_auprc))\n",
    "            \n",
    "            tb_x = epoch_index * len(train_dl) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            tb_writer.add_scalar('AUROC/train', last_auroc, tb_x)\n",
    "            tb_writer.add_scalar('AUPRC/train', last_auprc, tb_x)\n",
    "\n",
    "            running_loss, running_auroc, running_auprc = 0., 0., 0.\n",
    "\n",
    "    return last_loss, last_auroc, last_auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 0.3640937826037407 AUROC: 0.4978493360900233 AUPRC 0.04019666839390993\n",
      "  batch 200 loss: 0.3567062449455261 AUROC: 0.5002339681014681 AUPRC 0.04265733860433102\n",
      "LOSS train 0.3567062449455261 valid 0.3518361747264862\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.3485053241252899 AUROC: 0.5330621386292935 AUPRC 0.09099245145916939\n",
      "  batch 200 loss: 0.34563091427087783 AUROC: 0.5523177213955225 AUPRC 0.12381462626159191\n",
      "LOSS train 0.34563091427087783 valid 0.3469122648239136\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 0.3462478846311569 AUROC: 0.5745103447740134 AUPRC 0.16376675069332122\n",
      "  batch 200 loss: 0.3448654851317406 AUROC: 0.5809879383083006 AUPRC 0.17731387987732888\n",
      "LOSS train 0.3448654851317406 valid 0.34588274359703064\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 0.34352359145879746 AUROC: 0.5914790873418186 AUPRC 0.19990815415978433\n",
      "  batch 200 loss: 0.347372088432312 AUROC: 0.5945324449981854 AUPRC 0.2071671338379383\n",
      "LOSS train 0.347372088432312 valid 0.346902459859848\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 0.3452100187540054 AUROC: 0.5987966290584226 AUPRC 0.21730914637446402\n",
      "  batch 200 loss: 0.34360256969928743 AUROC: 0.601166293676983 AUPRC 0.22267135739326477\n",
      "LOSS train 0.34360256969928743 valid 0.34585040807724\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/p19/p19_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "best_vloss = 1e6\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    rd_cls.train(True)\n",
    "    avg_loss, avg_auroc, avg_auprc = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    running_vloss, running_vauroc, running_vauprc = 0., 0., 0.\n",
    "    rd_cls.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dl):\n",
    "            ts_inp, times, mask, static_inp, labels = vdata\n",
    "            voutputs, reg_loss = rd_cls(ts_inp, times, mask, static_inp)\n",
    "            vloss = loss_fn(voutputs, labels, reg_loss)\n",
    "            # vloss = loss_fn(voutputs, labels)\n",
    "            bin_voutputs = voutputs.argmax(dim=-1)\n",
    "            bin_auroc_metric.update(bin_voutputs, labels)\n",
    "            bin_auprc_metric.update(bin_voutputs, labels)\n",
    "\n",
    "            running_vloss += vloss\n",
    "            running_vauroc += bin_auroc_metric.compute()\n",
    "            running_vauprc += bin_auprc_metric.compute()\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    avg_vauroc = running_vauroc / (i + 1)\n",
    "    avg_vauprc = running_vauprc / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation AUROC',\n",
    "                    { 'Training' : avg_auroc, 'Validation' : avg_vauroc },\n",
    "                    epoch_number + 1)\n",
    "    writer.add_scalars('Training vs. Validation AUPRC',\n",
    "                    { 'Training' : avg_auprc, 'Validation' : avg_vauprc },\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    writer.flush()\n",
    "\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = './models/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(rd_cls.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rd_model.sensor_embed_ln(O->I): 2.648601e-03 -> 1.644914e-03\n",
      "rd_model.sensor_embed_map(O->I): 1.082606e-03 -> 6.291155e-04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gfa = GradientFlowAnalyzer(gt)\n",
    "\n",
    "flow = [\n",
    "    'rd_model.sensor_embed_ln',\n",
    "    'rd_model.sensor_embed_map',\n",
    "]\n",
    "\n",
    "print(pretty_flow(gfa.flow(flow, 0)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
